<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Wenming&#39;s Blog</title>
    <link>https://wmingj.github.io/</link>
    <description>Recent content on Wenming&#39;s Blog</description>
    <generator>Hugo -- 0.128.0</generator>
    <language>zh</language>
    <lastBuildDate>Mon, 12 Aug 2024 21:49:04 +0800</lastBuildDate>
    <atom:link href="https://wmingj.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>在Go中如何访问和修改私有对象</title>
      <link>https://wmingj.github.io/posts/get-set-private-object-in-go/</link>
      <pubDate>Mon, 12 Aug 2024 21:49:04 +0800</pubDate>
      <guid>https://wmingj.github.io/posts/get-set-private-object-in-go/</guid>
      <description>我们都知道，基本上所有主流编程语言都支持对变量、类型以及函数方法等设置私有或公开，从而帮助程序员设计出封装优秀的模块。然而，实际开发中，难免需要使用第三方包的私有函数或方法，或修改其中的私有变量、熟悉等。在可观测性数据采集器开发中，由于集成了很多采集插件，经常需要魔改其中的代码，因此我对Go语言中如何修改这些私有对象的方式做了一个总结，以供后续参考。
方式方法 修改指针 指针本质上就是一个内存地址，这种方式下，我们通过对指针的计算（如果你有C/C++的经验，想必对指针运算一定有所耳闻），从而找到目标对象的内存地址，进而可以获取并修改指针所指向对象的值。
Examples：
// pa/a.go package pa type ExportedType struct { intField int stringField string flag bool } func (t *ExportedType) String() string { return fmt.Sprintf(&amp;#34;ExportedType{flag: %v}&amp;#34;, t.flag) } // main/main.go func main() { et := &amp;amp;pa.ExportedType{} fmt.Printf(&amp;#34;before edit: %s\n&amp;#34;, et) ptr := unsafe.Pointer(et) // line 1 flagPtr := unsafe.Pointer(uintptr(ptr) + unsafe.Sizeof(0) + unsafe.Sizeof(&amp;#34;&amp;#34;)) // line 2 flagField := (*bool)(flagPtr) // line 3 *flagField = true // line 4 fmt.</description>
    </item>
    <item>
      <title>ClickHouse的分布式实现</title>
      <link>https://wmingj.github.io/posts/clickhouse-distrubuted-arch/</link>
      <pubDate>Mon, 10 Apr 2023 16:24:04 +0800</pubDate>
      <guid>https://wmingj.github.io/posts/clickhouse-distrubuted-arch/</guid>
      <description>当我们需要在实际生产环境中使用ClickHouse时，高可用与可扩展是绕不开的话题，因此ClickHouse也提供了分布式的相关机制来应对这些问题。在下文中，我们将主要从副本机制、分片机制两个个方面来对齐进行介绍。
副本机制 ClickHouse通过扩展MergeTree为ReplicatedMergeTree来创建副本表引擎（通过在MergeTree添加Replicated前缀来表示副本表引擎）。这里需要注意的是，副本表并非一种具体的表引擎，而是一种逻辑上的表引擎，实际数据的存取仍然通过MergeTree来完成。
注意：这里，我们假定集群名为local，且包含两个节点chi-0和chi-1
建表 ReplicatedMergeTree通过类似如下语句进行创建：
CREATE TABLE table_name ( EventDate DateTime, CounterID UInt32, UserID UInt32, ver UInt16 ) ENGINE = ReplicatedReplacingMergeTree(&amp;#39;/clickhouse/tables/{cluster}-{shard}/table_name&amp;#39;, &amp;#39;{replica}&amp;#39;, ver) PARTITION BY toYYYYMM(EventDate) ORDER BY (CounterID, EventDate, intHash32(UserID)) SAMPLE BY intHash32(UserID); 有两个参数需要重点说明一下，分别为zoo_path和replica_name参数：
zoo_path: 表示表所在的zk路径 replica_name: 表示副本名称，通常为主机名 ClickHouse会在zk中建立路径zoo_path，并在zoo_path的子目录/replicas下根据replica_name创建副本标识，因此可以看到replica_name参数的作用主要就是用来作为副本ID。
我们这里假定首先在chi-0节点上执行了建表语句
其首先创建一个副本实例，进行一些初始化的工作，在zk上创建相关节点 接着在/replicas节点下注册副本实例chi-0 启用监听任务，监听/log节点 参与leader节点选举(通过向/leader_election写入数据，谁先写入成功谁就是leader) 接着，我们在chi-1节点上执行建表语句：
首先也是创建副本实例，进行初始化工作 接着在/replicas节点下注册副本实例chi-1 启用监听任务，监听/log节点 参与leader节点选举(此时由于chi-0节点上已经执行过建表流程了，因此chi-0为leader副本) /log节点非常重要，用来记录各种操作LogEntry包括获取part，合并part，删除分区等等操作
写入 接着，我们通过执行INSERT INTO语句向chi-0节点写入数据（当写入请求被发到从节点时，从节点会将其转发到主节点）。
此时，会首先在本地完成分区数据的写入，然后向/blocks节点写入该分区的block_id
block是ClickHouse中的最小数据单元，这里在/blocks节点中写入block_id主要是为了后续数据的去重
接着向/log节点推送日志，日志信息如下所示：
format version: 4 create_time: 2022-09-04 14:30:58 source replica: chi-0 block_id: 20220904_5211346952104599192_1472622755444261990 get 20220904_269677_269677_0 part_type: Compact .</description>
    </item>
    <item>
      <title>Clickhouse MergeTree解读</title>
      <link>https://wmingj.github.io/posts/clickhouse-mergetree/</link>
      <pubDate>Thu, 22 Dec 2022 23:21:05 +0800</pubDate>
      <guid>https://wmingj.github.io/posts/clickhouse-mergetree/</guid>
      <description>在众多的ClickHouse表引擎中，当属MergeTree（合并树）最为常用也最为完备，适用于中绝大部分场景，因此搞懂MergeTree对与理解ClickHouse至关重要！
在本文中，我将通过主要从数据模型、数据写入、数据读取3个方面来阐述MergeTree的实现
本文需要读者具备一定的ClickHouse使用经验，譬如建表、写入、查询等
数据模型 在MergeTree引擎底层实现中，从上至下主要有以下3种数据模型组成：Part、Block、PrimaryKey
Part 这里需要注意的时，Part不是Partition，对于一张表来说：
Part是用来存储一组行对应于，在磁盘上对应于一个数据目录，目录里有列数据、索引等信息 Partition则是一种虚拟的概念，在磁盘上没有具体的表示，不过可以说某个Partition包含多个Part 在建表的DDL中，我们可通过PARTITION BY参数来配置分区规则，ClickHouse会根据分区规则生成不同分区ID，从而在写入时将数据落盘到对应分区中。一但有数据写入，ClickHouse则根据分区ID创建对应的Part目录。
其中目录的命名规则为{PartiionID}_{MinBlockNum}_{MaxBlockNum}_{Level}：
PartiionID：即为分区ID MinBlockNum：表示最小数据块编号，后续解释 MaxBlockNum：表示最大数据块编号，后续解释 Level：表示该Part被合并过的次数，对于每个新建Part目录而言，其初始值为0，每合并一次则累积加1 目录中的文件主要包括如下部分：
数据相关：{Column}.mrk、{Column}.mrk2、{Column}.bin、primary.idx(mrk, mrk2应该是版本不同) 二级索引相关：skp_idx_{Column}.idx、skp_idx_{Column}.mrk 此外，每个Part在逻辑上被划分为多个粒度（粒度大小由index_granularity或index_granularity_bytes控制）；而在物理上，列数据则被划分为多个数据块。
Block Block即为数据块，在内存中由三元组(列数据，列类型，列名)组成。是ClickHouse中的最小数据处理单元，例如，在查询过程中，数据是一个块接着一个块被处理的。
而在磁盘上，其则通过排序、压缩序列化后生成压缩数据块并存储于{Column}.bin中，其中表示如下所示：
其中，头信息(Header)部分包含3种信息：
CompressionMethod：Uint8，压缩方法，如LZ4, ZSTD CompressedSize：UInt32，压缩后的字节大小 UncompressedSize：UInt32，压缩前的字节大小 其中每个数据块的大小都会被控制在64K-1MB的范围内（由min_compress_block_size和max_compress_block_size指定）。
这里我们为什么要将{Column}.bin划分成多个数据块呢？其目的主要包括：
数据压缩后虽然可以显著减少数据大小，但是解压缩会带来性能损耗，因此需要控制被压缩数据的大小，以求性能与压缩率之间的平衡（这条我也不太理解，还请评论区大佬指教:)） 当读取数据时，需要将数据加载到内存中再解压，通过压缩数据块，我们可以不用加载整个.bin文件，从而进一步降低读取范围 PrimaryKey 主键索引(Primary Key)是一张表不可或缺的一部分，你可以不指定，但是这会导致每次查询都是全表扫描从而几乎不可用。
PrimaryKey主要是由{Column}.mrk，primary.idx和{Column}.bin三者协同实现，其中：
primary.idx：保存主键与标记的映射关系 {Column}.mrk：保存标记与数据块偏移量的映射关系 {Column}.bin：保存数据块 具体实现可以参考我之前的文章
数据写入 ClickHouse的数据写入流程是比较简单直接的，整体流程如下图所示：
每收到写入请求，ClickHouse就会生成一个新的Part目录，接着按index_granularity定义的粒度将数据划分，并依次进行处理，生成primary.idx文件，针对每一行生成.mrk和.bin文件。
合并 写入结束后，ClickHouse的后台线程会周期性地选择一些Part进行合并，合并后数据依然有序。
在上文中，我们提到的MinBlockNum此时会取各个part中的MinBlockNum最小值，而MaxBlockNum则会取各个part中的MinBlockNum最小值。例如201403_1_1_0和201403_2_2_0合并后，生成的新part目录为201403_1_2_1。
查询 查询的过程本质上可以看做是不断缩小数据扫描的过程，流程如下图所示：
当ClickHouse收到查询请求时，其会首先尝试定位到具体的分区，然后扫描所有的part，然后通过part目录中的一级、二级索引定位到标记，再通过标记找到压缩数据块，并将其加载到内存中进行处理。
此外，为了提升查询性能，ClickHouse还是用了vectorized query execution和以及少量runtime code generation技术，从CPU层面提升性能（这块内容比较多，这里就不详解了，后续我将尝试再写一篇博客来介绍）。
总结 本文，我们首先从数据模型层面自顶向下分别介绍了分区、Part、Block、PrimaryKey，它们构建起了MergeTree的总体框架。然后，我们分别介绍了数据写入与数据查询流程，将数据模型串联起来，并详细介绍了它们之间是如何相互协同的。
总体看来，MergeTree实现上还是比较简单易懂的，希望本文能对你有所帮助
参考 https://stackoverflow.com/questions/60142967/how-to-understand-part-and-partition-of-clickhouse https://clickhouse.com/docs/en/intro/ 《ClickHouse原理解析与应用实践》 </description>
    </item>
    <item>
      <title>ClickHouse跳数索引解读</title>
      <link>https://wmingj.github.io/posts/clickhouse-skipping-index/</link>
      <pubDate>Wed, 26 Oct 2022 23:20:05 +0800</pubDate>
      <guid>https://wmingj.github.io/posts/clickhouse-skipping-index/</guid>
      <description>在ClickHouse稀疏索引原理解读文章中，我们通过设置合理的稀疏主键索引，极大地优化了通用场景下的查询性能。然而，我们也发现，当我们想通过别的列（标签）进行过滤时，由于未能命中稀疏索引，就变成了全表扫描。
例如，对于cpu_ts表，当我们要通过别的维度，例如hostname，进行查询分析时，ClickHouse会对hostname列的所有值进行全表扫描，再根据WHERE中的条件对值进行过滤。
在传统的关系型数据库中，我们可以通过创建一个或多个的二级索引(B+树实现)来加快查询效率。而ClickHouse中也同样提供了一种类似的方式，不过由于ClickHouse是纯列式存储，磁盘上并没有单独的行数据，因此没法利用二级索引来构建面向行的索引。
因此ClickHouse通过一种被称为跳数索引的索引机制来达到传统二级索引的效果，之所以叫跳数，是因为数据的定位是通过跳过那些肯定不满足过滤条件的数据块来实现的。
通过hostname进行查询 添加跳数索引前 在cpu_ts表中，hostname字段每1024行就会重新生产一份随机字符串用来模拟实际场景。这里我们使用如下SQL查询主机fa9c19a5-39eb-4bea-97df-5a6b82e5e947的CPU使用率：
select ts, avg(usage_user) from cpu_ts where hostname = &amp;#39;fa9c19a5-39eb-4bea-97df-5a6b82e5e947&amp;#39; group by toStartOfMinute(timestamp) as ts order by ts; 结果如下：
Query id: e34e54e8-8c9f-4f5b-ba29-83f54a095167 ┌──────────────────ts─┬───avg(usage_user)─┐ │ 2022-01-15 07:24:00 │ 77.7843137254902 │ │ 2022-01-15 07:25:00 │ 75.23333333333333 │ │ 2022-01-15 07:26:00 │ 73.96666666666667 │ │ 2022-01-15 07:27:00 │ 79.7 │ │ 2022-01-15 07:28:00 │ 77.68333333333334 │ │ 2022-01-15 07:29:00 │ 73.03333333333333 │ │ 2022-01-15 07:30:00 │ 72.21666666666667 │ │ 2022-01-15 07:31:00 │ 75.</description>
    </item>
    <item>
      <title>ClickHouse稀疏索引原理解读</title>
      <link>https://wmingj.github.io/posts/clickhouse-sparse-index/</link>
      <pubDate>Sat, 13 Aug 2022 16:23:14 +0800</pubDate>
      <guid>https://wmingj.github.io/posts/clickhouse-sparse-index/</guid>
      <description>问个问题，如何优化一条SQL语句？我们首先想到的肯定是建索引。对于ClickHouse也不例外，尤其是稀疏主键索引（类似传统数据库中的主键索引）对性能的影响非常大。在下文中，我将结合例子对稀疏主键索引进行详细解读。
注：本文内容主要参考官方文档，如果有余力，强烈建议先行阅读
数据准备 这里，我将就我比较熟悉的时序数据进行举例。首先通过如下SQL建表：
-- create dabase create database test; use test; -- create table CREATE TABLE cpu ( `hostname` String, `reginon` String, `datacenter` String, `timestamp` DateTime64(9,&amp;#39;Asia/Shanghai&amp;#39;) CODEC (DoubleDelta), `usage_user` Float64, `usage_system` Float64 ) ENGINE = MergeTree() PRIMARY KEY tuple(); optimize table cpu final ; 并将本地的样本数据导入：
cat example/output.csv |clickhouse-client -d test -q &amp;#39;INSERT into cpu FORMAT CSV&amp;#39; output.csv是时序数据样本，时间间隔为1秒，包含了从2022-01-01 08:00:00到2022-01-15 07:59:59一共1209600条记录
optimize table 会强制进行merge之类的操作，使其达到最终状态
查询某段时间范围内的CPU使用率 SQL如下:
select ts, avg(usage_user) from cpu where timestamp &amp;gt; &amp;#39;2022-01-15 06:59:59.</description>
    </item>
    <item>
      <title>聊一聊可观测性</title>
      <link>https://wmingj.github.io/posts/what-is-observability/</link>
      <pubDate>Mon, 11 Jul 2022 23:51:39 +0800</pubDate>
      <guid>https://wmingj.github.io/posts/what-is-observability/</guid>
      <description>不知为何，可观测性这个词突然就火起来，各个大厂都甚至纷纷成立可观测团队了。那么可观测性究竟是个啥呢？它凭什么能火起来呢？它与传统监控有什么区别呢，是否又是新瓶装旧酒？
说实话，虽然我很久之前就听说过可观测性这个词，不过一直持怀疑态度，认为不过是炒概念。直到最近，读了《Observability Engineering》这本书，让我打开新视界，可观测性并非空穴来风，其确实有它的价值，而且相当大。
什么是可观测性 可观测性(Observability)这个单词最开始是出现于数学领域，表示度量一个系统内部状态由其外部输出的信息中推断出来的程度。Wikipedia上的定义有点抽象，举个汽车的例子：
一辆汽车可以说是一个系统，当其外部输出信息只有油箱油量、当前车速时，我们能推断出车辆预计还能行驶多少公里 而当其外部输出信息还包括发动机温度、轮胎磨损程度时，我们还能推断出发动机和轮胎的工作情况是否需要维护，如果需要维护那么能行驶的距离显然会比之前预估的行驶举例少，显然此时这辆汽车的可观测性程度比之更高了 而对于一个软件系统，它所能暴露的外部信息常见的有指标、日志、链路追踪事件，显然我们可以通一些工具来分析它的外部信息从而推断系统的内部状态。因此这里，软件系统暴露的信息越详细，分析工具越强劲，内部状态就能推断地越准确与详细，那么该软件系统的可观测性就越好。
不过，此时你可能有疑问：这些指标日志啥的不都是些的现成东西吗？那是不是可以说，本质上，传统意义的监控就是可观测性呢？这里的答案是否定的，且听我下文解释。
与传统监控的区别 传统意义上的监控(Monitoring)是一种行为，这种行为分析系统的内部状态。而可观测性正如上文中描述的一样，是一种系统的性质，其他性质有健壮性、可测试性等
在传统监控的场景下，我们收集、存储并分析指标数据。我们制作监控大盘并设置各种告警，当发生异常时，告警触发,收到告警后，我们根据告警的内容以及相关的监控图表，来推断出异常发生的原因，并由此做出相应的处理（或增加资源、或修复Bug）。
但是这里有一个很大前提，就是告警策略必须预先设置，而如何设置又完全取决于经验与直觉。换句话说，通过监控我们只能检测一些已知的潜在风险，例如机器的负载、CPU使用率、磁盘使用率等。而对于一个未知或复杂的系统，当它发生异常时，我们往往只能束手无策，或者通过一些线索去猜测可能的原因并验证，如果猜错了那又得重复上述过程，非常的浪费时间。
而在可观测性的场景下，系统中植入了各种各样的代码和工具，并提供了非常丰富的可观测的数据（metric, logs, traces等各种数据），通过这些数据并结合合适的工具，我们能够很快地排查出问题的根因所在。举个例子，同样是一个未知的系统，当发现某个接口很慢时，我们可以通过链路追踪工具找到瓶颈点，通过瓶颈点再分析当时的系统资源使用率，饱和度，负载情况以及应用日志等，从而很快地定位出根因（资源问题？代码问题？第三方服务问题？等等）
流行的原因 近10年IT相关行业发生了天翻地覆的变化。IT技术也是日新月异，尤其是微服务架构、分布式以及云原生的高速发展，以及各种敏捷开发思想深入人心。
如今的软件系统已经与10年前的大不相同了，复杂度、灵活度、变化度等都大幅提升。而由此带来的问题就是，系统稳定性保障变得越来越困难，尤其是问题根因的定位
例如，对于一些复杂问题，有时候花费数个月都无法定位，最后的选择往往都是推倒重来
因此单靠传统的监控已经无法满足当下软件系统的观测需求，传统监控只能解决那些&amp;quot;known unknown/known&amp;quot;类的问题，而无法应对&amp;quot;unknown unknown&amp;quot;类的问题，而这类问题在如今的架构下要多得多。
Known unknown/known：指的是你熟悉的已知或未知的软件系统，这种系统可预测，因此我们可以预先设置各种告警
Unknown unknown：指的是你不熟悉的未知系统，这种系统完全未知，只能通过可观测性工具来探测
三大支柱是可观测性吗 一说到可观测性，可能最先联想的就是“三大支柱(the three pillars)”，即logs, metrics以及traces（如下图所示）。很多人（包括我）经常以为它们就是所谓的可观测性，毕竟很多PAAS平台和厂商就是这么宣传的，但这并不完全正确。
是的，没错，三大支柱确实是可观测性体系里不可或缺的条件。但这并不代表我暴露了这些数据，我的软件系统就具备了可观测性，同样也不能代表我收集分析了这些数据，我就实现了一个可观测性工具系统。
首先，可观测性需要的数据并非只有这三者，它还可以是用户的反馈信息、系统profiling信息等各种统计、事件信息；其次暴露的数据的维度、基数以及数据间的关联度等等都会影响系统的可观测性；而一个可观测性工具的搭建除了收集这些统计、时间信息，还包括数据传输与处理，数据存储以及交互的易用性等，此外涉及到的数据隔离，容量规划，低成本且高性能等问题也是十分棘手的。
不过，话虽如此，“三大支柱”虽不能等同于可观测性，但它们是你迈向可观测性的第一步:)
总结 综上所述，如果你的应用非常简单，比如一个单体应用，那么传统监控也足够满足需求了。但是一旦切换为微服务架构，甚至完全云原生化的开发方式时，此时软件系统的复杂度就成指数级增加了，而此时可观测性就显得异常重要。
相信你都经过，一个软件系统随着业务的发展会变得越来越复杂，到最后每个人都只能往上面堆功能，而对老代码甚至不敢改动一行，最终软件系统就会变成人们口中的“屎山”，而后面的人的唯一选择只能是推到重来。
而如果可观测性一开始就在架构考虑中，那么无论我们的的系统变化多大，多复杂，我们都能对其了如指掌并快速定位问题根因，此外还能提前发现到系统架构的不合理之处并及时调整。
参考 Observability Engineering
https://www.splunk.com/en_us/data-insider/what-is-observability.html
https://en.wikipedia.org/wiki/Observability
https://www.dynatrace.com/news/blog/what-is-observability-2/</description>
    </item>
    <item>
      <title>使用Hugo部署GithubPages</title>
      <link>https://wmingj.github.io/posts/hugo-with-githubpages/</link>
      <pubDate>Sun, 29 May 2022 16:08:13 +0800</pubDate>
      <guid>https://wmingj.github.io/posts/hugo-with-githubpages/</guid>
      <description>记录一下使用Hugo生成静态博客，并通过githubPages自动化部署的过程。
这里，我的目标是：
使用blog-source作为原始的内容仓库，&amp;lt;your-name&amp;gt;.github.io作为实际的githubPages仓库 通过github Action将两者串联起来，原始内容提交变更时，自动触发内容生成并发布 这样的好处是，可以将blog-source作为私有仓库，并能直接以&amp;lt;your-name&amp;gt;.github.io作为URL。且通过github action实现CICD，解放双手实现自动化。这里我画了一张图，便于理解：
Hugo 安装Hugo，然后初始化
# macOS install hugo brew install hugo # create site project hugo new site blog-source 选择你中意的主题并安装
cd blog-source git init # add paperMod as theme git submodule add https://github.com/adityatelange/hugo-PaperMod themes/paperMod 添加文章并启动demo
hugo new posts/my-first-post.md # start demo for preview hugo server -D 创建一个额外的仓库，这里我创建一个名为blog-source的仓库并作为刚才创建的blog-source的远端仓库
cd blog-source git init git remote add origin &amp;lt;your-remove-git&amp;gt; GithubPages 创建一个githubPages仓库，名称必须是&amp;lt;your-name&amp;gt;.github.io。DOC
Connection 创建sshKey: ssh-keygen -t rsa -b 4096 -C &amp;quot;$(git config user.</description>
    </item>
    <item>
      <title>重启博客之路</title>
      <link>https://wmingj.github.io/posts/restart-my-blog/</link>
      <pubDate>Thu, 26 May 2022 00:01:42 +0800</pubDate>
      <guid>https://wmingj.github.io/posts/restart-my-blog/</guid>
      <description>不知为何，写博客总是断断续续，距离上次更新已过去快一年了，直至如今面试碰壁方才后悔莫及。
事实上，写博客对于对于知识的小伙理解非常有好处，毕竟要让别人听得懂，首先自己得更懂才行嘛。因为，通常学习一项新知识，通常需要理论学习-实践-总结输出三个阶段，而我往往只完成了第一阶段便草草了事，无法对知识有更深入的理解。因此，我打算重启我的博客之路，持续学习持续输出。
之前博客用过Hexo，也用过博客园等等，最近看到hugo的paperMod主题非常讨喜，因此打算彻底切换到hugo+paperMod，也算起个好头吧。之前的文章也会从博客园迁移到hugo上，不过后续两边也会尽量同步更新。
然后，也打算支持中英文双语，主要是为了提升自己的英文水平，以便能和世界上的程序员更好地交流。不过目前主打还是中文，母语写起来还是方便点，英文会挑选文章进行编写翻译，同时英文文章也会同步发布在Medium上。
此外，为了降低断更的概率、提高文章质量，我在这里给自己立一个flag，即每月至少更新一篇文章，文章长度适宜、做到通俗易懂、绝不模棱两可故作高深</description>
    </item>
    <item>
      <title>浅析Go内存分配器的实现</title>
      <link>https://wmingj.github.io/posts/memory-allocator-in-go/</link>
      <pubDate>Tue, 15 Jun 2021 21:08:09 +0800</pubDate>
      <guid>https://wmingj.github.io/posts/memory-allocator-in-go/</guid>
      <description>为什么需要内存分配器？ 总说周知，内存作为一种相对稀缺的资源，在操作系统中以虚拟内存的形式来作为一种内存抽象提供给进程，这里可以简单地把它看做一个连续的地址集合{0, 1, 2, ..., M}，由栈空间、堆空间、代码片、数据片等地址空间段组合而成，如下图所示(出自CS:APP3e, Bryant and O&amp;rsquo;Hallaron的第9章第9节)
这里我们重点关注Heap（堆），堆是一块动态的虚拟内存地址空间。在C语言中，我们通常使用malloc来申请内存以及使用free来释放内存，也许你想问，这样不就足够了吗？但是，这种手动的内存管理会带来很多问题，比如：
给程序员带来额外的心智负担，必须得及时释放掉不再使用的内存空间，否则就很容易出现内存泄露 随着内存的不断申请与释放，会产生大量的内存碎片，这将大大降低内存的利用率 因此，正确高效地管理内存空间是非常有必要的，常见的技术实现有Sequential allocation, Free-List allocation等。那么，在Go中，内存是如何被管理的呢？
注：此为Go1.13.6的实现逻辑，随版本更替某些细节会有些许不同
实现原理 Go的内存分配器是基于TCMalloc设计的，因此我建议你先行查阅，这将有利于理解接下来的内容。
大量工程经验证明，程序中的小对象占了绝大部分，且生命周期都较为短暂。因此，Go将内存划分为各种类别(Class)，并各自形成Free-List。相较于单一的Free-List分配器，分类后主要有以下优点：
其一方面减少不必要的搜索时间，因为对象只需要在其所属类别的空闲链表中搜索即可
另一方面减少了内存碎片化，同一类别的空闲链表，每个对象分配的空间都是一样大小(不足则补齐)，因此该链表除非无空闲空间，否则总能分配空间，避免了内存碎片
那么，Go内存分配器具体是如何实现的呢？接下来，我将以自顶向下的方式，从宏观到微观，层层拨开她的神秘面纱。
数据结构 首先，介绍Go内存分配中相关的数据结构。其总体概览图如下所示：
heapArena 在操作系统中，我们一般把堆看做是一块连续的虚拟内存空间。
Go将其划分为数个相同大小的连续空间块，称之arena，其中，heapArena则作为arena空间的管理单元，其结构如下所示：
type heapArena struct { bitmap [heapArenaBitmapBytes]byte spans [pagesPerArena]*mspan ... } bitmap: 表示arena区域中的哪些地址保存了对象，哪些地址保存了指针 spans: 表示arena区域中的哪些操作系统页(8K)属于哪些mspan mheap 然后，则是核心角色mheap了，它是Go内存管理中的核心数据结构，作为全局唯一变量，其结构如下所示：
type mheap struct { free mTreap ... allspans []*mspan ... arenas [1 &amp;lt;&amp;lt; arenaL1Bits]*[1 &amp;lt;&amp;lt; arenaL2Bits]*heapArena ... central [numSpanClasses]struct { mcentral mcentral pad [cpu.CacheLinePadSize - unsafe.Sizeof(mcentral{})%cpu.CacheLinePadSize]byte } } free: 使用树堆的结构来保存各种类别的空闲mspan allspans: 用以记录了分配过了的mspan arenas: 表示其覆盖的所有arena区域，通过虚拟内存地址计算得到下标索引 central: 表示其覆盖的所有mcentral，一共134个，对应67个类别 mcentral 而mcentral充当mspan的中心管理员，负责管理某一类别的mspan，其结构如下：</description>
    </item>
    <item>
      <title>Golang内存优化实践指南</title>
      <link>https://wmingj.github.io/posts/memory-optimize-best-practice-for-golang/</link>
      <pubDate>Sat, 09 Jan 2021 19:52:52 +0000</pubDate>
      <guid>https://wmingj.github.io/posts/memory-optimize-best-practice-for-golang/</guid>
      <description>最近做了许多有关Go内存优化的工作，总结了一些定位、调优方面的套路和经验，于是，想通过这篇文章与大家分享讨论。
发现问题 性能优化领域有一条总所周知的铁律，即：不要过早地优化。编写一个程序，首先应该保证其功能的正确性，以及诸如设计是否合理、需求等是否满足，过早地优化只会引入不必要的复杂度以及设计不合理等各种问题。
那么何时才能开始优化呢？一句话，问题出现时。诸如程序出现频繁OOM，CPU使用率异常偏高等情况。如今，在这微服务盛行的时代，公司内部都会拥有一套或简单或复杂的监控系统，当系统给你发出相关告警时，你就要开始重视起来了。
问题定位 1. 查看内存曲线 首先，当程序发生OOM时，首先应该查看程序的内存使用量曲线，可以通过现有监控系统查看，或者prometheus之类的开源工具。
曲线一般都是呈上升趋势，比如goroutine泄露的曲线一般是使用量缓慢上升直至OOM，而内存分配不合理往往时在高负载时快速攀升以致OOM。
2. 问题复现 这块是可选项，但是最好能保证复现。如果能在本地或debug环境复现问题，这将非常有利于我们反复进行测试和验证。
3. 使用pprof定位 Go官方工具提供了pporf来专门用以性能问题定位，首先得在程序中开启pprof收集功能，这里假定问题程序已开启pprof。(对这块不够了解的同学，建议通过这两篇文章(1, 2)学习下pprof工具的基本用法)
接下来，我们复现问题场景，并及时获取heap和groutine的采样信息。
获取heap信息: curl http://loalhost:6060/debug/pprof/heap -o h1.out 获取groutine信息：curl http://loalhost:6060/debug/pprof/goroutine -o g1.out 这里你可能想问，这样就够了吗？
当然不是，只获取一份样本信息是不够的。内存使用量是不断变化的(通常是上升)，因此我们需要的也是期间heap、gourtine信息的变化信息，而非瞬时值。一般来说，我们需要一份正常情况下的样本信息，一份或多份内存升高期间的样本信息。
数据收集完毕后，我们按照如下3个方面来排查定位。
排查goroutine泄露 使用命令go tool pprof --base g1.out g2.out ，比较goroutine信息来判断是否有goroutine激增的情况。
进入交互界面后，输入top命令，查看期间goroutine的变化。
同时可执行go tool pprof --base g2.out g3.out来验证。我之前写了的一篇实战文章，记录了goroutine泄露的排查过程。
排查内存使用量 使用命令go tool pprof --base h1.out h2.out，比较当前堆内存的使用量信息来判断内存使用量。
进入交互界面后，输入top命令，查看期间堆内存使用量的变化。
排查内存分配量 当上述排查方向都没发现问题时，那就要查看期间是否有大量的内存申请了，以至于GC都来不及回收。使用命令go tool pprof --alloc_space --base h1.out h2.out，通过比较前后内存分配量来判断是否有分配不合理的现象。
进入交互界面后，输入top命令，查看期间堆内存分配量的变化。
一般来说，通过上述3个方面的排查，我们基本就能定位出究竟是哪方面的问题导致内存激增了。我们可以通过web命令，更为直观地查看问题函数(方法)的完整调用链。
问题优化 定位到问题根因后，接下来就是优化阶段了。这个阶段需要对Go本身足够熟悉，还得对问题程序的业务逻辑有所了解。
我梳理了一些常见的优化手段，仅供参考。实际场景还是得实际分析。
goroutine泄露 这种问题还是比较好修复的，需要显式地保证goroutine能正确退出，而非以一些自以为的假设来保证。例如，通过传递context.Context对象来显式退出
go func(ctx context.Context) { for { select { case &amp;lt;-ctx.</description>
    </item>
    <item>
      <title>一次抓包排查实战记录</title>
      <link>https://wmingj.github.io/posts/tcpdump-pricate-record/</link>
      <pubDate>Sun, 02 Aug 2020 11:00:43 +0000</pubDate>
      <guid>https://wmingj.github.io/posts/tcpdump-pricate-record/</guid>
      <description>问题的发现 周五，本是一个风清气爽，令人愉悦的日子。我本还在美滋滋地等待着下班，然而天有不测，有用户反应容器日志看不到了，根据经验我知道，日志采集&amp;amp;收集链路上很可能又发生了阻塞。
登录目标容器所在机器找到日志采集容器，并娴熟地敲下docker logs --tail 200 -f &amp;lt;container-id&amp;gt;命令，发现确实阻塞了，阻塞原因是上报日志的请求500了，从而不断重试导致日志采集阻塞。
随后，我找到收集端的容器，查看日志，发现确实有请求报500了，并且抛出了Unknown value type的错误，查看相关代码。
业务代码：
if _, err := jsonparser.ArrayEach(body, func(value []byte, dataType jsonparser.ValueType, offset int, err error) { ... }); err != nil { return err // 错误抛出点 } jsonparser包中代码：
显然问题出在了对body的解析上，究竟是什么样的body导致了解析错误呢？接下来，就是tcpdump和wireshark上场的时候了。
使用Tcpdump抓包 首先，我们通过tcpdump抓到相关的请求。由于日志采集端会不断重试，因此最简单的方法便是登录采集端所在机器，并敲下如下命令tcpdump -i tunl0 dst port 7777 -w td.out ，并等待10-20秒。
熟悉tcpdump的小伙伴，对这条命令显然已经心领神会了。尽管如此，这里我还是稍微解释下。
-i tunl0：-i 参数用来指定网卡，由于采集器并没有通过eth0。因此，实战中，有时发现命令正确缺抓不到包的情况时，不妨指定下别的网卡。网络错综复杂，不一定都会通过eth0网卡。 dst port 777： 指定了目标端口作为过滤参数，收集端程序的端口号是7777 -w td.out: 表明将抓包记录保存在td.out文件中，这是因为json body是用base64编码并使用gzip加密后传输的，因此我得使用wireshark来抽离出来。（主要还是wireshark太香了:)，界面友好，操作简单，功能强大） 接着，我用scp命令将td.out文件拷到本地。并使用wireshar打开它
使用Wireshark分析 打开后，首先映入眼帘的则是上图内容，看起来很多？不要慌，由于我们排查的是http请求，在过滤栏里输入HTTP，过滤掉非HTTP协议的记录。
我们可以很清楚地发现，所有的HTTP都是发往一个IP的，且长度都是59，显然这些请求都是日志采集端程序不断重试的请求。接下来，我们只需要将某个请求里的body提取出来查看即可。
很幸运，wireshark提供了这种功能，如上图所示，我们成功提取出来body内容。为bnVsbA==，使用base64解码后为null。
解决问题 既然body的内容为null，那么调用jsonparser.ArrayEach报错也是意料之中的了，body内容必须得是一个JsonArray。
然而，采集端为何会发送body为null的请求呢，深入源码，发现了如下一段逻辑。
func (e *jsonEncoder) encode(obj []publisher.</description>
    </item>
    <item>
      <title>Golang中的map实现</title>
      <link>https://wmingj.github.io/posts/map-in-go/</link>
      <pubDate>Sat, 01 Feb 2020 17:28:15 +0000</pubDate>
      <guid>https://wmingj.github.io/posts/map-in-go/</guid>
      <description>总所周知，大多数语言中，字典的底层是哈希表，而且其算法也是十分清晰的。无论采用链表法还是开放寻址法，我们都能实现一个简单的哈希表结构。对于Go来说，它是具体如何实现哈希表的呢？以及，采取了哪些优化策略呢？
内存模型 map在内存的总体结构如下图所示。
头部结构体hmap type hmap struct { count int // 键值对个数 flags uint8 B uint8 // 2^B = 桶数量 noverflow uint16 // 溢出桶的个数 hash0 uint32 // hash seed buckets unsafe.Pointer // 哈希桶 oldbuckets unsafe.Pointer // 原哈希桶，扩容时为非空 nevacuate uintptr // 扩容进度，地址小于它的桶已被迁移了 extra *mapextra // optional fields } hmap即为map编译后的内存表示，这里需要注意的有两点。
B的值是根据负载因子(LoadFactor)以及存储的键值对数量，在创建或扩容时动态改变 buckets是一个指针，它指向一个bmap结构 桶结构体bmap type bmap struct { // tophash数组可以看做键值对的索引 tophash [bucketCnt]uint8 // 实际上编译器会动态添加下述属性 // keys [8]keytype // values [8]valuetype // padding uinptr // overflow uinptr } 虽然bmap结构体中只有一个tophash数组，但实际上，其后跟着8个key的槽位、8个value的槽位、padding以及一个overflow指针。如下图所示</description>
    </item>
    <item>
      <title>用Golang实现并理解Web中间件</title>
      <link>https://wmingj.github.io/posts/understand-middleware/</link>
      <pubDate>Fri, 20 Dec 2019 14:39:21 +0000</pubDate>
      <guid>https://wmingj.github.io/posts/understand-middleware/</guid>
      <description>&lt;p&gt;在编写web应用中，我们常常会遇到这样的需求，比如，我们需要上报每个API的运行时间到运维监控系统。这时候你可以像下述代码一样将统计的逻辑写到每个路由函数中。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Golang中的string实现</title>
      <link>https://wmingj.github.io/posts/string-in-golang/</link>
      <pubDate>Wed, 11 Dec 2019 21:31:47 +0000</pubDate>
      <guid>https://wmingj.github.io/posts/string-in-golang/</guid>
      <description>&lt;p&gt;说到&lt;code&gt;string&lt;/code&gt;类型，我们往往都能很熟练地对它进行各种处理，包括迭代、随机访问和匹配等等操作。然而在工作中，我发现迭代一个字符串产生的字符的类型与随机访问一个字符的类型却并不相同，为什么会这么奇怪呢？于是我决定一探究竟&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>https://wmingj.github.io/readings/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://wmingj.github.io/readings/</guid>
      <description>仅以记录我在读、已读、预读之书。
25年 《凤凰架构》 《System Design Interview - An Insider’s Guide》 《重构 第二版》 《Learn Concurrent Programming with Go (James Cutajar)》：介绍了Go并发编程中的常见问题、编程范式和解决规避方法 24年 [10%] 《C Primer Plus, 6th Edition (Stephen Prata) 》：写ebpf用 《Learning eBPF (Liz Rice) (Z-Library)》 90%：详细介绍eBPF相关理论和实践，值得一读 23.01-23.04 《认知觉醒》：认知科学的入门吧，读了之后，降低了焦虑与不安，充实起来 《BPF Performance Tools》：学一下BPF、eBPF 22.06-22.12 《知识变现》：技术人还是看看，学习下如何提升影响力 《Observability Engineering》：关于可观测领域比较全面的书了，从业者可重点关注第1、2、4章节 《stop reading news》：确实很有道理，这年头新闻量与日俱增，浪费时间且易过度焦虑 《深入理解Kafka：核心设计与实践原理 第二版》：工作中常用Kafka，讲得还不错，有理论有实操（60%） 《ClickHouse原理解析与应用实践》：主要看了实现原理部分，讲的还是比较通俗易懂的，值得一读（50%） 《Systems Performance: Enterprise and the Cloud》：性能之巅第二版，[真]大头书，重要读了概念部分，实操部分待日后实践的时候再细读(60%) 《Streaming Processing with Apache Flink》： 快速过了一遍，后续深度使用的时候再回头看看 《如何阅读一本》：完全颠覆了我的阅读观，从前阅读效率真的是太低了，强烈推荐！（100%） 《重构：第二版》：略读。最重要的点：小步重构、持续集成 《What is eBPF》：略读。eBPF的由来、使用以及应用常见简介 《为什么中国人勤劳却不富有》：略读。介绍了中国经济高速发展的原因及所面临的机遇与困难，用勤劳来对冲制度成本终究不是长久之计! 《100 Go Mistakes and How to Avoid Them》：想不到大道至简的Go也有这么多细节，其中至少有50%的内容刷新了我的认知，非常适合有一定Go使用经验的程序员进一步提升自己 已读 《Real-Time Analytics Techniques to Analyze》：70% 介绍了流式系统的常见设计和考虑，还是不错的，对我设计监控数据链路有帮助</description>
    </item>
    <item>
      <title>About</title>
      <link>https://wmingj.github.io/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://wmingj.github.io/about/</guid>
      <description>嗨，我的朋友！
欢迎来到姜闻名的技术博客！
我的座右铭是Keep It Simple, Stupid！
我是一名软件工程师，Go是我最喜欢的开发语言，专注于 Observability, Cloud Native and Backend
最后，欢迎与我联系 :P</description>
    </item>
  </channel>
</rss>
