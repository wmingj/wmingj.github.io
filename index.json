[{"content":"当我们需要在实际生产环境中使用ClickHouse时，高可用与可扩展是绕不开的话题，因此ClickHouse也提供了分布式的相关机制来应对这些问题。在下文中，我们将主要从副本机制、分片机制两个个方面来对齐进行介绍。\n副本机制 ClickHouse通过扩展MergeTree为ReplicatedMergeTree来创建副本表引擎（通过在MergeTree添加Replicated前缀来表示副本表引擎）。这里需要注意的是，副本表并非一种具体的表引擎，而是一种逻辑上的表引擎，实际数据的存取仍然通过MergeTree来完成。\n 注意：这里，我们假定集群名为local，且包含两个节点chi-0和chi-1\n 建表 ReplicatedMergeTree通过类似如下语句进行创建：\nCREATE TABLE table_name (  EventDate DateTime,  CounterID UInt32,  UserID UInt32,  ver UInt16 ) ENGINE = ReplicatedReplacingMergeTree(\u0026#39;/clickhouse/tables/{cluster}-{shard}/table_name\u0026#39;, \u0026#39;{replica}\u0026#39;, ver) PARTITION BY toYYYYMM(EventDate) ORDER BY (CounterID, EventDate, intHash32(UserID)) SAMPLE BY intHash32(UserID); 有两个参数需要重点说明一下，分别为zoo_path和replica_name参数：\n zoo_path: 表示表所在的zk路径 replica_name: 表示副本名称，通常为主机名   ClickHouse会在zk中建立路径zoo_path，并在zoo_path的子目录/replicas下根据replica_name创建副本标识，因此可以看到replica_name参数的作用主要就是用来作为副本ID。\n 我们这里假定首先在chi-0节点上执行了建表语句\n 其首先创建一个副本实例，进行一些初始化的工作，在zk上创建相关节点 接着在/replicas节点下注册副本实例chi-0 启用监听任务，监听/log节点 参与leader节点选举(通过向/leader_election写入数据，谁先写入成功谁就是leader)  接着，我们在chi-1节点上执行建表语句：\n 首先也是创建副本实例，进行初始化工作 接着在/replicas节点下注册副本实例chi-1 启用监听任务，监听/log节点 参与leader节点选举(此时由于chi-0节点上已经执行过建表流程了，因此chi-0为leader副本)   /log节点非常重要，用来记录各种操作LogEntry包括获取part，合并part，删除分区等等操作\n 写入 接着，我们通过执行INSERT INTO语句向chi-0节点写入数据（当写入请求被发到从节点时，从节点会将其转发到主节点）。\n此时，会首先在本地完成分区数据的写入，然后向/blocks节点写入该分区的block_id\n block是ClickHouse中的最小数据单元，这里在/blocks节点中写入block_id主要是为了后续数据的去重\n 接着向/log节点推送日志，日志信息如下所示：\nformat version: 4 create_time: 2022-09-04 14:30:58 source replica: chi-0 block_id: 20220904_5211346952104599192_1472622755444261990 get 20220904_269677_269677_0 part_type: Compact ... 日志内容中主要包含了源节点chi-0，操作类型get，分区目录20220904_269677_269677_0，以及block_id。\n接下来就轮到副本节点chi-1的回合了，由于chi-1也监听了/log节点，通过分析日志信息，它需要执行获取part的操作。因此它获取到日志信息后，会将其推送到/replicas/chi-1/queue/节点下，稍后在执行。\n 把日志推送到队列中，而不是立马执行。主要是处于性能的考虑，比如同一时间段可能会收到很多日志信息，可以需要将其攒批处理以提升性能\n 随后，chi-1节点从队列中获取任务开始执行，其首先从/replicas获取所有副本，选择一个合适的副本(chi-0)，对其发起数据下载的请求\n 选择一个合适的副本，往往需要考虑副本拥有数据的新旧程度以及副本节点的负载\n chi-0收到下载请求后，发送相关part的数据给chi-1，chi-1完成写入\n合并 合并操作本质上也是由各个副本独立完成的，不会涉及到任何part数据的传输。首先，合并操作可以在任何节点上触发，但是都必须由主节点来发布合并任务。\n 假设，在从节点chi-1上，我们触发了合并请求（可通过OPTIMIZE操作触发） 此时，chi-1不会立马执行合并操作，而是向主节点chi-0发送请求，并由chi-0来指定合并计划 chi-0将合并计划生成操作日志推送到/log下 所有副本(包括主副本)监听到操作日志后，将合并任务推送到各自的/queue下 副本各自监听/queue，收到合并任务后，各自分别执行合并任务  查询 当客户端发起查询请求时，由于设计多个副本，因此需要考虑负载均衡的问题。对此，ClickHouse会根据配置负载均衡算法来选择一个合适的副本，由load_babalancing参数决定。\n分片机制 为解决可扩展性的问题，ClickHouse引入了分片机制，主要通过Distributed引擎来定义，DDL如下所示：\nCREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] (  name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],  name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],  ... ) ENGINE = Distributed(cluster, database, table[, sharding_key[, policy_name]]) [SETTINGS name=value, ...] 参数说明如下所示：\n cluster：表示集群名 database：表示所管辖的本地表的数据库名 table：表示所管辖的本地表的表明 sharding_key：表示分片键，  通过Distributed定义的分布式表，同样也是虚拟表，实际数据由其管理的本地表存储，如图所示：\n分片规则 分片规则主要是用以路由数据，决定数据存储在哪个分片中，规则主要由sharding_key和weight参数决定：\n  sharding_key：用来生成分片的key\n  weight：表示分片权重，通过服务端的配置来设置。其可以用来调整分片之间数据的分布，默认都是1。若某个分片的weight越大，则数据会向该分片倾斜。\n  假定只有0号和1号两个分片，其中weight分别为10和20，sharding_key为rand()（生成随机整数），则最终的表达式为如下所示:\n shard_number = rand() % 30\n当shard_number位于[0, 10)之间时，数据被路由到0号分片\n当shard_number位于[10, 30)之间时，数据则被路由到1号分片\n 写入 通过上述的分片规则，我们就可以确定写入请求要被写入到那个节点上了，此时再对该节点发起请求。这种方式是非常高效也是官方的推荐方式，因为其直接在客户端决定了数据流向，无需额外的路由操作，不过缺点就是客户端实现比较复杂。\n除此之外，我们还可以直接往Distributed表里插入数据，此时数据的分片和路由则由ClickHouse节点代理了，当数据不属于当前节点上的分片时，则需要将数据发送到目标分片所在的节点上，从而会导致额外的网络传输，影响性能。\n查询 与写入不同的是，分布式查询是直接查询Distributed表。\n 节点收到查询请求后，节点将分布式查询转换为本地表查询，并将查询下发到各个分片上 各个分片各自执行本地表查询，然后将查询结果返回节点 最后，节点将结果集合并成最终结果，返回给客户端  我们通过EXPLAIN语句可以看到，分布式查询会分别从两个分片中获取数据并合并\n小心子查询 当涉及到子查询时，需要特别小心，因为很容易导致结果错误或者性能下降。\n 这里，我们假定一个4节点的集群，users_all和orders_all分别为本地表users和orders的分布式表，单副本双分片\n 结果错误 select username from users_all where id IN (select distinct(user_id) from orders)  ch-0首先将该SQL转换成本地SQL：select username from users where id IN (select distinct(user_id) from orders)，然后下发本地SQL到users的分片节点上执行 分片节点得到子结果后，将结果返回给ch-0，ch-0合并并生成最终结果。  看起来没问题？其实并非如此，此时得到的结果是不正确的。\n 假若orders表的分片在ch-2和ch-3节点上，而orders表的分片在ch-0和ch-1上，则子查询的结果均为空，因此最终结果也为空   假若orders表的分片在ch-2和ch-3节点上，而orders表的分片在ch-0和ch-2上，则两个子查询中的只有ch-2节点上的子查询能得到结果，因此最终结果仅为正确结果的一部分  这是因为子查询为本地表查询，若节点上不包含orders的分片数据，那么就只会得到空数据，因此子查询语句同样需要改为分布式查询\n性能下降 此时，SQL则会变成\nselect username from users_all where id IN (select distinct(user_id) from orders_all)   ch-0首先将该SQL转换成本地SQL：select username from users where id IN (select distinct(user_id) from orders_all)，然后下发到users所在的分片节点上执行\n  分片节点再将其子查询转换为本地SQL：select distinct(user_id) from orders下发到orders所在的分片节点上执行，得到结果集\n  orders所在的分片节点分别执行分布式查询得到结果集，再执行原SQL得到子结果集，返回给ch-0，ch-0合并并生成最终结果。\n  可以明显看到，子查询在分片ch-2和ch-3上分别执行了分布式查询（ch-2下发子查询的本地SQL到ch-3，而ch-3下发子查询的本地SQL到ch-2），同样的结果集被查询了两次，从而导致查询性能下降，且orders的分片数越多，性能下降越明显。\n幸运的是，ClickHouse提供了GOBAL IN语句来解决此类子查询问题，此时ClickHouse会先单独执行子查询，得到的结果存在一个临时内存表里，并将内存表的数据发送到users的分片节点上。\n总结 在本文中，我们分别介绍了ClickHouse是如何利用副本机制用以解决高可用问题场景，以及利用分片机制用以解决可扩展性问题。\n总的来说，ClickHouse的采用了多主架构，避免了主从架构的单点问题，将负载均摊到各个节点，并且给予用户足够的灵活度，例如可以将本地表转换为分布式表、将分布式表转换成本地表、调整权重控制数据倾斜度等等。\n然而，其缺点也比较明显，一则配置比较繁琐，需要较多的人工运维操作；二则，严重依赖zookeeper，基本所有操作都需要zookeeper来进行信息的同步与交换，当存在很多表时会对zookeeper产生较大压力，从而影响整体集群的性能。\n参考  https://clickhouse.com/docs/en/development/architecture/#replication https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/replication https://clickhouse.com/docs/en/engines/table-engines/special/distributed  ","permalink":"https://erenming.github.io/posts/clickhouse-distrubuted-arch/","summary":"当我们需要在实际生产环境中使用ClickHouse时，高可用与可扩展是绕不开的话题，因此ClickHouse也提供了分布式的相关机制来应对这些问题。在下文中，我们将主要从副本机制、分片机制两个个方面来对齐进行介绍。\n副本机制 ClickHouse通过扩展MergeTree为ReplicatedMergeTree来创建副本表引擎（通过在MergeTree添加Replicated前缀来表示副本表引擎）。这里需要注意的是，副本表并非一种具体的表引擎，而是一种逻辑上的表引擎，实际数据的存取仍然通过MergeTree来完成。\n 注意：这里，我们假定集群名为local，且包含两个节点chi-0和chi-1\n 建表 ReplicatedMergeTree通过类似如下语句进行创建：\nCREATE TABLE table_name (  EventDate DateTime,  CounterID UInt32,  UserID UInt32,  ver UInt16 ) ENGINE = ReplicatedReplacingMergeTree(\u0026#39;/clickhouse/tables/{cluster}-{shard}/table_name\u0026#39;, \u0026#39;{replica}\u0026#39;, ver) PARTITION BY toYYYYMM(EventDate) ORDER BY (CounterID, EventDate, intHash32(UserID)) SAMPLE BY intHash32(UserID); 有两个参数需要重点说明一下，分别为zoo_path和replica_name参数：\n zoo_path: 表示表所在的zk路径 replica_name: 表示副本名称，通常为主机名   ClickHouse会在zk中建立路径zoo_path，并在zoo_path的子目录/replicas下根据replica_name创建副本标识，因此可以看到replica_name参数的作用主要就是用来作为副本ID。\n 我们这里假定首先在chi-0节点上执行了建表语句\n 其首先创建一个副本实例，进行一些初始化的工作，在zk上创建相关节点 接着在/replicas节点下注册副本实例chi-0 启用监听任务，监听/log节点 参与leader节点选举(通过向/leader_election写入数据，谁先写入成功谁就是leader)  接着，我们在chi-1节点上执行建表语句：\n 首先也是创建副本实例，进行初始化工作 接着在/replicas节点下注册副本实例chi-1 启用监听任务，监听/log节点 参与leader节点选举(此时由于chi-0节点上已经执行过建表流程了，因此chi-0为leader副本)   /log节点非常重要，用来记录各种操作LogEntry包括获取part，合并part，删除分区等等操作\n 写入 接着，我们通过执行INSERT INTO语句向chi-0节点写入数据（当写入请求被发到从节点时，从节点会将其转发到主节点）。","title":"ClickHouse的分布式实现"},{"content":"在众多的ClickHouse表引擎中，当属MergeTree（合并树）最为常用也最为完备，适用于中绝大部分场景，因此搞懂MergeTree对与理解ClickHouse至关重要！\n在本文中，我将通过主要从数据模型、数据写入、数据读取3个方面来阐述MergeTree的实现\n 本文需要读者具备一定的ClickHouse使用经验，譬如建表、写入、查询等\n 数据模型 在MergeTree引擎底层实现中，从上至下主要有以下3种数据模型组成：Part、Block、PrimaryKey\nPart 这里需要注意的时，Part不是Partition，对于一张表来说：\n Part是用来存储一组行对应于，在磁盘上对应于一个数据目录，目录里有列数据、索引等信息 Partition则是一种虚拟的概念，在磁盘上没有具体的表示，不过可以说某个Partition包含多个Part  在建表的DDL中，我们可通过PARTITION BY参数来配置分区规则，ClickHouse会根据分区规则生成不同分区ID，从而在写入时将数据落盘到对应分区中。一但有数据写入，ClickHouse则根据分区ID创建对应的Part目录。\n其中目录的命名规则为{PartiionID}_{MinBlockNum}_{MaxBlockNum}_{Level}：\n PartiionID：即为分区ID MinBlockNum：表示最小数据块编号，后续解释 MaxBlockNum：表示最大数据块编号，后续解释 Level：表示该Part被合并过的次数，对于每个新建Part目录而言，其初始值为0，每合并一次则累积加1  目录中的文件主要包括如下部分：\n 数据相关：{Column}.mrk、{Column}.mrk2、{Column}.bin、primary.idx(mrk, mrk2应该是版本不同) 二级索引相关：skp_idx_{Column}.idx、skp_idx_{Column}.mrk  此外，每个Part在逻辑上被划分为多个粒度（粒度大小由index_granularity或index_granularity_bytes控制）；而在物理上，列数据则被划分为多个数据块。\nBlock Block即为数据块，在内存中由三元组(列数据，列类型，列名)组成。是ClickHouse中的最小数据处理单元，例如，在查询过程中，数据是一个块接着一个块被处理的。\n而在磁盘上，其则通过排序、压缩序列化后生成压缩数据块并存储于{Column}.bin中，其中表示如下所示：\n其中，头信息(Header)部分包含3种信息：\n CompressionMethod：Uint8，压缩方法，如LZ4, ZSTD CompressedSize：UInt32，压缩后的字节大小 UncompressedSize：UInt32，压缩前的字节大小  其中每个数据块的大小都会被控制在64K-1MB的范围内（由min_compress_block_size和max_compress_block_size指定）。\n这里我们为什么要将{Column}.bin划分成多个数据块呢？其目的主要包括：\n 数据压缩后虽然可以显著减少数据大小，但是解压缩会带来性能损耗，因此需要控制被压缩数据的大小，以求性能与压缩率之间的平衡（这条我也不太理解，还请评论区大佬指教:)） 当读取数据时，需要将数据加载到内存中再解压，通过压缩数据块，我们可以不用加载整个.bin文件，从而进一步降低读取范围  PrimaryKey 主键索引(Primary Key)是一张表不可或缺的一部分，你可以不指定，但是这会导致每次查询都是全表扫描从而几乎不可用。\nPrimaryKey主要是由{Column}.mrk，primary.idx和{Column}.bin三者协同实现，其中：\n primary.idx：保存主键与标记的映射关系 {Column}.mrk：保存标记与数据块偏移量的映射关系 {Column}.bin：保存数据块   具体实现可以参考我之前的文章\n 数据写入 ClickHouse的数据写入流程是比较简单直接的，整体流程如下图所示：\n每收到写入请求，ClickHouse就会生成一个新的Part目录，接着按index_granularity定义的粒度将数据划分，并依次进行处理，生成primary.idx文件，针对每一行生成.mrk和.bin文件。\n合并 写入结束后，ClickHouse的后台线程会周期性地选择一些Part进行合并，合并后数据依然有序。\n在上文中，我们提到的MinBlockNum此时会取各个part中的MinBlockNum最小值，而MaxBlockNum则会取各个part中的MinBlockNum最小值。例如201403_1_1_0和201403_2_2_0合并后，生成的新part目录为201403_1_2_1。\n查询 查询的过程本质上可以看做是不断缩小数据扫描的过程，流程如下图所示：\n当ClickHouse收到查询请求时，其会首先尝试定位到具体的分区，然后扫描所有的part，然后通过part目录中的一级、二级索引定位到标记，再通过标记找到压缩数据块，并将其加载到内存中进行处理。\n此外，为了提升查询性能，ClickHouse还是用了vectorized query execution和以及少量runtime code generation技术，从CPU层面提升性能（这块内容比较多，这里就不详解了，后续我将尝试再写一篇博客来介绍）。\n总结 本文，我们首先从数据模型层面自顶向下分别介绍了分区、Part、Block、PrimaryKey，它们构建起了MergeTree的总体框架。然后，我们分别介绍了数据写入与数据查询流程，将数据模型串联起来，并详细介绍了它们之间是如何相互协同的。\n总体看来，MergeTree实现上还是比较简单易懂的，希望本文能对你有所帮助\n参考  https://stackoverflow.com/questions/60142967/how-to-understand-part-and-partition-of-clickhouse https://clickhouse.com/docs/en/intro/ 《ClickHouse原理解析与应用实践》  ","permalink":"https://erenming.github.io/posts/clickhouse-mergetree/","summary":"在众多的ClickHouse表引擎中，当属MergeTree（合并树）最为常用也最为完备，适用于中绝大部分场景，因此搞懂MergeTree对与理解ClickHouse至关重要！\n在本文中，我将通过主要从数据模型、数据写入、数据读取3个方面来阐述MergeTree的实现\n 本文需要读者具备一定的ClickHouse使用经验，譬如建表、写入、查询等\n 数据模型 在MergeTree引擎底层实现中，从上至下主要有以下3种数据模型组成：Part、Block、PrimaryKey\nPart 这里需要注意的时，Part不是Partition，对于一张表来说：\n Part是用来存储一组行对应于，在磁盘上对应于一个数据目录，目录里有列数据、索引等信息 Partition则是一种虚拟的概念，在磁盘上没有具体的表示，不过可以说某个Partition包含多个Part  在建表的DDL中，我们可通过PARTITION BY参数来配置分区规则，ClickHouse会根据分区规则生成不同分区ID，从而在写入时将数据落盘到对应分区中。一但有数据写入，ClickHouse则根据分区ID创建对应的Part目录。\n其中目录的命名规则为{PartiionID}_{MinBlockNum}_{MaxBlockNum}_{Level}：\n PartiionID：即为分区ID MinBlockNum：表示最小数据块编号，后续解释 MaxBlockNum：表示最大数据块编号，后续解释 Level：表示该Part被合并过的次数，对于每个新建Part目录而言，其初始值为0，每合并一次则累积加1  目录中的文件主要包括如下部分：\n 数据相关：{Column}.mrk、{Column}.mrk2、{Column}.bin、primary.idx(mrk, mrk2应该是版本不同) 二级索引相关：skp_idx_{Column}.idx、skp_idx_{Column}.mrk  此外，每个Part在逻辑上被划分为多个粒度（粒度大小由index_granularity或index_granularity_bytes控制）；而在物理上，列数据则被划分为多个数据块。\nBlock Block即为数据块，在内存中由三元组(列数据，列类型，列名)组成。是ClickHouse中的最小数据处理单元，例如，在查询过程中，数据是一个块接着一个块被处理的。\n而在磁盘上，其则通过排序、压缩序列化后生成压缩数据块并存储于{Column}.bin中，其中表示如下所示：\n其中，头信息(Header)部分包含3种信息：\n CompressionMethod：Uint8，压缩方法，如LZ4, ZSTD CompressedSize：UInt32，压缩后的字节大小 UncompressedSize：UInt32，压缩前的字节大小  其中每个数据块的大小都会被控制在64K-1MB的范围内（由min_compress_block_size和max_compress_block_size指定）。\n这里我们为什么要将{Column}.bin划分成多个数据块呢？其目的主要包括：\n 数据压缩后虽然可以显著减少数据大小，但是解压缩会带来性能损耗，因此需要控制被压缩数据的大小，以求性能与压缩率之间的平衡（这条我也不太理解，还请评论区大佬指教:)） 当读取数据时，需要将数据加载到内存中再解压，通过压缩数据块，我们可以不用加载整个.bin文件，从而进一步降低读取范围  PrimaryKey 主键索引(Primary Key)是一张表不可或缺的一部分，你可以不指定，但是这会导致每次查询都是全表扫描从而几乎不可用。\nPrimaryKey主要是由{Column}.mrk，primary.idx和{Column}.bin三者协同实现，其中：\n primary.idx：保存主键与标记的映射关系 {Column}.mrk：保存标记与数据块偏移量的映射关系 {Column}.bin：保存数据块   具体实现可以参考我之前的文章\n 数据写入 ClickHouse的数据写入流程是比较简单直接的，整体流程如下图所示：\n每收到写入请求，ClickHouse就会生成一个新的Part目录，接着按index_granularity定义的粒度将数据划分，并依次进行处理，生成primary.idx文件，针对每一行生成.mrk和.bin文件。\n合并 写入结束后，ClickHouse的后台线程会周期性地选择一些Part进行合并，合并后数据依然有序。\n在上文中，我们提到的MinBlockNum此时会取各个part中的MinBlockNum最小值，而MaxBlockNum则会取各个part中的MinBlockNum最小值。例如201403_1_1_0和201403_2_2_0合并后，生成的新part目录为201403_1_2_1。\n查询 查询的过程本质上可以看做是不断缩小数据扫描的过程，流程如下图所示：\n当ClickHouse收到查询请求时，其会首先尝试定位到具体的分区，然后扫描所有的part，然后通过part目录中的一级、二级索引定位到标记，再通过标记找到压缩数据块，并将其加载到内存中进行处理。\n此外，为了提升查询性能，ClickHouse还是用了vectorized query execution和以及少量runtime code generation技术，从CPU层面提升性能（这块内容比较多，这里就不详解了，后续我将尝试再写一篇博客来介绍）。","title":"Clickhouse MergeTree解读"},{"content":"在ClickHouse稀疏索引原理解读文章中，我们通过设置合理的稀疏主键索引，极大地优化了通用场景下的查询性能。然而，我们也发现，当我们想通过别的列（标签）进行过滤时，由于未能命中稀疏索引，就变成了全表扫描。\n例如，对于cpu_ts表，当我们要通过别的维度，例如hostname，进行查询分析时，ClickHouse会对hostname列的所有值进行全表扫描，再根据WHERE中的条件对值进行过滤。\n在传统的关系型数据库中，我们可以通过创建一个或多个的二级索引(B+树实现)来加快查询效率。而ClickHouse中也同样提供了一种类似的方式，不过由于ClickHouse是纯列式存储，磁盘上并没有单独的行数据，因此没法利用二级索引来构建面向行的索引。\n因此ClickHouse通过一种被称为跳数索引的索引机制来达到传统二级索引的效果，之所以叫跳数，是因为数据的定位是通过跳过那些肯定不满足过滤条件的数据块来实现的。\n通过hostname进行查询 添加跳数索引前 在cpu_ts表中，hostname字段每1024行就会重新生产一份随机字符串用来模拟实际场景。这里我们使用如下SQL查询主机fa9c19a5-39eb-4bea-97df-5a6b82e5e947的CPU使用率：\nselect ts, avg(usage_user) from cpu_ts where hostname = \u0026#39;fa9c19a5-39eb-4bea-97df-5a6b82e5e947\u0026#39; group by toStartOfMinute(timestamp) as ts order by ts; 结果如下：\nQuery id: e34e54e8-8c9f-4f5b-ba29-83f54a095167 ┌──────────────────ts─┬───avg(usage_user)─┐ │ 2022-01-15 07:24:00 │ 77.7843137254902 │ │ 2022-01-15 07:25:00 │ 75.23333333333333 │ │ 2022-01-15 07:26:00 │ 73.96666666666667 │ │ 2022-01-15 07:27:00 │ 79.7 │ │ 2022-01-15 07:28:00 │ 77.68333333333334 │ │ 2022-01-15 07:29:00 │ 73.03333333333333 │ │ 2022-01-15 07:30:00 │ 72.21666666666667 │ │ 2022-01-15 07:31:00 │ 75.11666666666666 │ │ 2022-01-15 07:32:00 │ 80.28333333333333 │ │ 2022-01-15 07:33:00 │ 82.33333333333333 │ │ 2022-01-15 07:34:00 │ 80 │ │ 2022-01-15 07:35:00 │ 84.33333333333333 │ │ 2022-01-15 07:36:00 │ 85.98333333333333 │ │ 2022-01-15 07:37:00 │ 76.01666666666667 │ │ 2022-01-15 07:38:00 │ 67.51666666666667 │ │ 2022-01-15 07:39:00 │ 71.2 │ │ 2022-01-15 07:40:00 │ 78.11666666666666 │ │ 2022-01-15 07:41:00 │ 78.71428571428571 │ └─────────────────────┴───────────────────┘ 18 rows in set. Elapsed: 0.015 sec. Processed 1.21 million rows, 54.52 MB (82.17 million rows/s., 3.70 GB/s.) 可以看到，这里ClickHouse扫描12100万行，显然进行了一次全表扫描。我们执行EXPLAIN语句：\nexplain indexes=1 select ts, avg(usage_user) from cpu_ts where hostname = \u0026#39;fa9c19a5-39eb-4bea-97df-5a6b82e5e947\u0026#39; group by toStartOfMinute(timestamp) as ts order by ts; 结果如下：\nQuery id: 7cd58c55-3fc6-4394-9579-55edde163192  ┌─explain───────────────────────────────────────────────────────────────────────────────┐ │ Expression (Projection) │ │ MergingSorted (Merge sorted streams for ORDER BY) │ │ MergeSorting (Merge sorted blocks for ORDER BY) │ │ PartialSorting (Sort each block for ORDER BY) │ │ Expression (Before ORDER BY) │ │ Aggregating │ │ Expression (Before GROUP BY) │ │ SettingQuotaAndLimits (Set limits and quota after reading from storage) │ ��� ReadFromMergeTree │ │ Indexes: │ │ PrimaryKey │ │ Condition: true │ │ Parts: 1/1 │ │ Granules: 148/148 │ └───────────────────────────────────────────────────────────────────────────────────────┘ 可以看到，由于WHERE条件未指定timestamp，因此无法利用稀疏索引来减少扫描量。\n添加跳数索引后 接着，让我们通过如下ALTER语句给hostname列添加跳数索引:\nALTER TABLE test.cpu_ts ADD INDEX idx_hostname hostname TYPE set(100) GRANULARITY 2; 然后，我们再用同样的语句进行查询主机fa9c19a5-39eb-4bea-97df-5a6b82e5e947的CPU使用率，得到结果如下：\nQuery id: a9b7fc8d-bb76-4969-984c-d9561f5ff56b  ┌──────────────────ts─┬───avg(usage_user)─┐ │ 2022-01-15 07:24:00 │ 77.7843137254902 │ │ 2022-01-15 07:25:00 │ 75.23333333333333 │ │ 2022-01-15 07:26:00 │ 73.96666666666667 │ │ 2022-01-15 07:27:00 │ 79.7 │ │ 2022-01-15 07:28:00 │ 77.68333333333334 │ │ 2022-01-15 07:29:00 │ 73.03333333333333 │ │ 2022-01-15 07:30:00 │ 72.21666666666667 │ │ 2022-01-15 07:31:00 │ 75.11666666666666 │ │ 2022-01-15 07:32:00 │ 80.28333333333333 │ │ 2022-01-15 07:33:00 │ 82.33333333333333 │ │ 2022-01-15 07:34:00 │ 80 │ │ 2022-01-15 07:35:00 │ 84.33333333333333 │ │ 2022-01-15 07:36:00 │ 85.98333333333333 │ │ 2022-01-15 07:37:00 │ 76.01666666666667 │ │ 2022-01-15 07:38:00 │ 67.51666666666667 │ │ 2022-01-15 07:39:00 │ 71.2 │ │ 2022-01-15 07:40:00 │ 78.11666666666666 │ │ 2022-01-15 07:41:00 │ 78.71428571428571 │ └─────────────────────┴───────────────────┘  18 rows in set. Elapsed: 0.023 sec. Processed 13.57 thousand rows, 678.56 KB (583.11 thousand rows/s., 29.16 MB/s.) 可以看到，同样的过滤条件，此时仅仅扫描了13570行。我们用EXPLAIN语句分析：\nexplain indexes=1 select ts, avg(usage_user) from cpu_ts where hostname = \u0026#39;fa9c19a5-39eb-4bea-97df-5a6b82e5e947\u0026#39; group by toStartOfMinute(timestamp) as ts order by ts; 输出如下：\nQuery id: b0c15f16-9a04-4b87-a46d-f94f1df8273d ┌─explain───────────────────────────────────────────────────────────────────────────────┐ │ Expression (Projection) │ │ MergingSorted (Merge sorted streams for ORDER BY) │ │ MergeSorting (Merge sorted blocks for ORDER BY) │ │ PartialSorting (Sort each block for ORDER BY) │ │ Expression (Before ORDER BY) │ │ Aggregating │ │ Expression (Before GROUP BY) │ │ SettingQuotaAndLimits (Set limits and quota after reading from storage) │ ��� ReadFromMergeTree │ │ Indexes: │ │ PrimaryKey │ │ Condition: true │ │ Parts: 1/1 │ │ Granules: 148/148 │ │ Skip │ │ Name: idx_hostname │ │ Description: set GRANULARITY 2 │ │ Parts: 1/1 │ │ Granules: 1/148 │ └───────────────────────────────────────────────────────────────────────────────────────┘ 我们可以看到，尽管未命中稀疏索引，但是得益于跳数索引，ClickHouse仅仅只扫描1357行，整整减少了9000多倍的扫描量！\n实现原理 当我们条件跳数索引之后，在分区目录下会分别生成skp_idx_[IndexName].idx与skp_idx_[IndexName].mrk2文件\n-rw-r----- 1 clickhouse clickhouse 41386 Jul 25 15:44 skp_idx_idx_hostname.idx -rw-r----- 1 clickhouse clickhouse 1776 Jul 25 15:44 skp_idx_idx_hostname.mrk2 与稀疏索引一样，以.idx后缀的文件为索引文件，而以.mrk2后缀的文件则为标记文件。\n索引建立 跳数索引 建立过程中，ClickHouse会以GRANULARITY的值，每GRANULARITY个index_granularity(默认8192行)来生成一行跳数索引，这里对于索引idx_hostname，ClickHouse会以每两个索引粒度聚合生成一条映射关系，并存于skp_idx_idx_hostname.idx中：\n如图中所示，idx文件中保存了标记与value（根据索引类型执行不同的生成算法）的映射关系，当WHERE条件中包含跳数索引所关联的过滤条件时，就会根据value判断来进行剪枝。\n 这里与稀疏索引不同的是，跳数索引只能遍历idx文件中的所有记录（复杂度为O(n)）来剪枝，而稀疏索引则能通过二分查找（复杂度为O(logn)）来定位数据块\n 数据定位 与稀疏索引定位类似，ClickHouse会根据WHERE中的条件，对数据块进行剪枝（即跳过那些肯定不包含目标数据的数据），从而得到所有标记的集合，最后再通过加载标记集合所关联的数据块进行后续分析:\n一些注意事项 尽管从直觉上来看，跳数索引用法与传统关系型数据库的二级索引十分类似，但是其并不能像二级索引(b+数)那样直接定位出数据所在的行，只能通过跳过的方式来减少扫描量。\n在cpu_ts表中，若某个主机名(localhost)均匀地分布在整个时间范围内（极端情况下，每两个index_granularity就出现一次），那么当我们通过WHERE hostname='localhost'进行过滤时，则无法跳过任何数据块（即全表扫描）！\n我们重新生成一份样本数据并写入表cpu_ts_hostname，其中每1024行中会包好一行特殊记录起hostname为localhost，接着让我们执行如下语句：\nselect ts, avg(usage_user) from cpu_ts_hostname where hostname = \u0026#39;localhost\u0026#39; group by toStartOfMinute(timestamp) as ts order by ts; 得到结果：\n... │ 2022-01-15 05:24:00 │ 74 │ │ 2022-01-15 05:41:00 │ 83 │ │ 2022-01-15 05:58:00 │ 67 │ │ 2022-01-15 06:15:00 │ 67 │ │ 2022-01-15 06:32:00 │ 20 │ │ 2022-01-15 06:50:00 │ 45 │ │ 2022-01-15 07:07:00 │ 59 │ │ 2022-01-15 07:24:00 │ 81 │ │ 2022-01-15 07:41:00 │ 79 │ │ 2022-01-15 07:58:00 │ 29 │ └─────────────────────┴─────────────────┘  1181 rows in set. Elapsed: 0.023 sec. Processed 1.21 million rows, 73.75 MB (53.68 million rows/s., 3.27 GB/s.) 通过EXPLAIN分析：\nQuery id: cb51d2c0-28df-4ccf-98d2-2db4c1d9fb34  ┌─explain───────────────────────────────────────────────────────────────────────────────┐ │ Expression (Projection) │ │ MergingSorted (Merge sorted streams for ORDER BY) │ │ MergeSorting (Merge sorted blocks for ORDER BY) │ │ PartialSorting (Sort each block for ORDER BY) │ │ Expression (Before ORDER BY) │ │ Aggregating │ │ Expression (Before GROUP BY) │ │ SettingQuotaAndLimits (Set limits and quota after reading from storage) │ ��� ReadFromMergeTree │ │ Indexes: │ │ PrimaryKey │ │ Condition: true │ │ Parts: 2/2 │ │ Granules: 148/148 │ │ Skip │ │ Name: idx_hostname │ │ Description: set GRANULARITY 2 │ │ Parts: 2/2 │ │ Granules: 0/148 │ └───────────────────────────────────────────────────────────────────────────────────────┘ 可以看到，此时跳数索引未跳过任何一个数据块，对于这种情况，我们完全无法利用跳数索引来降低扫描量，而建立跳数索引造成的成本浪费则无法避免\n 这种情况在可观测性数据领域里还是比较普遍，标签的值大多不会随时间变化而变化，同样的数据也会随时间反复出现。常见的如集群名，主机名，租户ID等\n 因此我们需要对是否建立跳数索引保持谨慎，必须先仔细分析数据与PRIMARY KEY的关联度，并反复测试验证，否则建立跳数索引非但不能提升查询性能，反而会造成额外开销且影响写入性能。\n总结 本篇文章是ClickHouse稀疏索引原理解读的延伸，当稀疏索引无法满足业务，我们可以通过建立跳数索引来显著提高查询性能（当然还可以通过重建稀疏索引，物化表等方式）。\n同时，我们还介绍了跳数索引的简单用法、底层实现原理（原理上与稀疏索引类似），以及使用跳数索引时需要注意的事项，我们不应盲目地试图通过创建跳数来提升查询性能，而必须先仔细分析数据分布并反复测试！\n参考   Understanding ClickHouse Data Skipping Indexes\n  https://altinity.com/blog/clickhouse-black-magic-skipping-indices\n  ClickHouse原理解析与应用实践\n  ","permalink":"https://erenming.github.io/posts/clickhouse-skipping-index/","summary":"在ClickHouse稀疏索引原理解读文章中，我们通过设置合理的稀疏主键索引，极大地优化了通用场景下的查询性能。然而，我们也发现，当我们想通过别的列（标签）进行过滤时，由于未能命中稀疏索引，就变成了全表扫描。\n例如，对于cpu_ts表，当我们要通过别的维度，例如hostname，进行查询分析时，ClickHouse会对hostname列的所有值进行全表扫描，再根据WHERE中的条件对值进行过滤。\n在传统的关系型数据库中，我们可以通过创建一个或多个的二级索引(B+树实现)来加快查询效率。而ClickHouse中也同样提供了一种类似的方式，不过由于ClickHouse是纯列式存储，磁盘上并没有单独的行数据，因此没法利用二级索引来构建面向行的索引。\n因此ClickHouse通过一种被称为跳数索引的索引机制来达到传统二级索引的效果，之所以叫跳数，是因为数据的定位是通过跳过那些肯定不满足过滤条件的数据块来实现的。\n通过hostname进行查询 添加跳数索引前 在cpu_ts表中，hostname字段每1024行就会重新生产一份随机字符串用来模拟实际场景。这里我们使用如下SQL查询主机fa9c19a5-39eb-4bea-97df-5a6b82e5e947的CPU使用率：\nselect ts, avg(usage_user) from cpu_ts where hostname = \u0026#39;fa9c19a5-39eb-4bea-97df-5a6b82e5e947\u0026#39; group by toStartOfMinute(timestamp) as ts order by ts; 结果如下：\nQuery id: e34e54e8-8c9f-4f5b-ba29-83f54a095167 ┌──────────────────ts─┬───avg(usage_user)─┐ │ 2022-01-15 07:24:00 │ 77.7843137254902 │ │ 2022-01-15 07:25:00 │ 75.23333333333333 │ │ 2022-01-15 07:26:00 │ 73.96666666666667 │ │ 2022-01-15 07:27:00 │ 79.7 │ │ 2022-01-15 07:28:00 │ 77.68333333333334 │ │ 2022-01-15 07:29:00 │ 73.03333333333333 │ │ 2022-01-15 07:30:00 │ 72.21666666666667 │ │ 2022-01-15 07:31:00 │ 75.","title":"ClickHouse跳数索引解读"},{"content":"不知为何，可观测性这个词突然就火起来，各个大厂都甚至纷纷成立可观测团队了。那么可观测性究竟是个啥呢？它凭什么能火起来呢？它与传统监控有什么区别呢，是否又是新瓶装旧酒？\n 说实话，虽然我很久之前就听说过可观测性这个词，不过一直持怀疑态度，认为不过是炒概念。直到最近，读了《Observability Engineering》这本书，让我打开新视界，可观测性并非空穴来风，其确实有它的价值，而且相当大。\n 什么是可观测性 可观测性(Observability)这个单词最开始是出现于数学领域，表示度量一个系统内部状态由其外部输出的信息中推断出来的程度。Wikipedia上的定义有点抽象，举个汽车的例子：\n 一辆汽车可以说是一个系统，当其外部输出信息只有油箱油量、当前车速时，我们能推断出车辆预计还能行驶多少公里 而当其外部输出信息还包括发动机温度、轮胎磨损程度时，我们还能推断出发动机和轮胎的工作情况是否需要维护，如果需要维护那么能行驶的距离显然会比之前预估的行驶举例少，显然此时这辆汽车的可观测性程度比之更高了  而对于一个软件系统，它所能暴露的外部信息常见的有指标、日志、链路追踪事件，显然我们可以通一些工具来分析它的外部信息从而推断系统的内部状态。因此这里，软件系统暴露的信息越详细，分析工具越强劲，内部状态就能推断地越准确与详细，那么该软件系统的可观测性就越好。\n不过，此时你可能有疑问：这些指标日志啥的不都是些的现成东西吗？那是不是可以说，本质上，传统意义的监控就是可观测性呢？这里的答案是否定的，且听我下文解释。\n与传统监控的区别 传统意义上的监控(Monitoring)是一种行为，这种行为分析系统的内部状态。而可观测性正如上文中描述的一样，是一种系统的性质，其他性质有健壮性、可测试性等\n在传统监控的场景下，我们收集、存储并分析指标数据。我们制作监控大盘并设置各种告警，当发生异常时，告警触发,收到告警后，我们根据告警的内容以及相关的监控图表，来推断出异常发生的原因，并由此做出相应的处理（或增加资源、或修复Bug）。\n但是这里有一个很大前提，就是告警策略必须预先设置，而如何设置又完全取决于经验与直觉。换句话说，通过监控我们只能检测一些已知的潜在风险，例如机器的负载、CPU使用率、磁盘使用率等。而对于一个未知或复杂的系统，当它发生异常时，我们往往只能束手无策，或者通过一些线索去猜测可能的原因并验证，如果猜错了那又得重复上述过程，非常的浪费时间。\n而在可观测性的场景下，系统中植入了各种各样的代码和工具，并提供了非常丰富的可观测的数据（metric, logs, traces等各种数据），通过这些数据并结合合适的工具，我们能够很快地排查出问题的根因所在。举个例子，同样是一个未知的系统，当发现某个接口很慢时，我们可以通过链路追踪工具找到瓶颈点，通过瓶颈点再分析当时的系统资源使用率，饱和度，负载情况以及应用日志等，从而很快地定位出根因（资源问题？代码问题？第三方服务问题？等等）\n流行的原因 近10年IT相关行业发生了天翻地覆的变化。IT技术也是日新月异，尤其是微服务架构、分布式以及云原生的高速发展，以及各种敏捷开发思想深入人心。\n如今的软件系统已经与10年前的大不相同了，复杂度、灵活度、变化度等都大幅提升。而由此带来的问题就是，系统稳定性保障变得越来越困难，尤其是问题根因的定位\n 例如，对于一些复杂问题，有时候花费数个月都无法定位，最后的选择往往都是推倒重来\n 因此单靠传统的监控已经无法满足当下软件系统的观测需求，传统监控只能解决那些\u0026quot;known unknown/known\u0026quot;类的问题，而无法应对\u0026quot;unknown unknown\u0026quot;类的问题，而这类问题在如今的架构下要多得多。\n Known unknown/known：指的是你熟悉的已知或未知的软件系统，这种系统可预测，因此我们可以预先设置各种告警\nUnknown unknown：指的是你不熟悉的未知系统，这种系统完全未知，只能通过可观测性工具来探测\n 三大支柱是可观测性吗 一说到可观测性，可能最先联想的就是“三大支柱(the three pillars)”，即logs, metrics以及traces（如下图所示）。很多人（包括我）经常以为它们就是所谓的可观测性，毕竟很多PAAS平台和厂商就是这么宣传的，但这并不完全正确。\n是的，没错，三大支柱确实是可观测性体系里不可或缺的条件。但这并不代表我暴露了这些数据，我的软件系统就具备了可观测性，同样也不能代表我收集分析了这些数据，我就实现了一个可观测性工具系统。\n首先，可观测性需要的数据并非只有这三者，它还可以是用户的反馈信息、系统profiling信息等各种统计、事件信息；其次暴露的数据的维度、基数以及数据间的关联度等等都会影响系统的可观测性；而一个可观测性工具的搭建除了收集这些统计、时间信息，还包括数据传输与处理，数据存储以及交互的易用性等，此外涉及到的数据隔离，容量规划，低成本且高性能等问题也是十分棘手的。\n不过，话虽如此，“三大支柱”虽不能等同于可观测性，但它们是你迈向可观测性的第一步:)\n总结 综上所述，如果你的应用非常简单，比如一个单体应用，那么传统监控也足够满足需求了。但是一旦切换为微服务架构，甚至完全云原生化的开发方式时，此时软件系统的复杂度就成指数级增加了，而此时可观测性就显得异常重要。\n相信你都经过，一个软件系统随着业务的发展会变得越来越复杂，到最后每个人都只能往上面堆功能，而对老代码甚至不敢改动一行，最终软件系统就会变成人们口中的“屎山”，而后面的人的唯一选择只能是推到重来。\n而如果可观测性一开始就在架构考虑中，那么无论我们的的系统变化多大，多复杂，我们都能对其了如指掌并快速定位问题根因，此外还能提前发现到系统架构的不合理之处并及时调整。\n参考   Observability Engineering\n  https://www.splunk.com/en_us/data-insider/what-is-observability.html\n  https://en.wikipedia.org/wiki/Observability\n  https://www.dynatrace.com/news/blog/what-is-observability-2/\n  ","permalink":"https://erenming.github.io/posts/what-is-observability/","summary":"不知为何，可观测性这个词突然就火起来，各个大厂都甚至纷纷成立可观测团队了。那么可观测性究竟是个啥呢？它凭什么能火起来呢？它与传统监控有什么区别呢，是否又是新瓶装旧酒？\n 说实话，虽然我很久之前就听说过可观测性这个词，不过一直持怀疑态度，认为不过是炒概念。直到最近，读了《Observability Engineering》这本书，让我打开新视界，可观测性并非空穴来风，其确实有它的价值，而且相当大。\n 什么是可观测性 可观测性(Observability)这个单词最开始是出现于数学领域，表示度量一个系统内部状态由其外部输出的信息中推断出来的程度。Wikipedia上的定义有点抽象，举个汽车的例子：\n 一辆汽车可以说是一个系统，当其外部输出信息只有油箱油量、当前车速时，我们能推断出车辆预计还能行驶多少公里 而当其外部输出信息还包括发动机温度、轮胎磨损程度时，我们还能推断出发动机和轮胎的工作情况是否需要维护，如果需要维护那么能行驶的距离显然会比之前预估的行驶举例少，显然此时这辆汽车的可观测性程度比之更高了  而对于一个软件系统，它所能暴露的外部信息常见的有指标、日志、链路追踪事件，显然我们可以通一些工具来分析它的外部信息从而推断系统的内部状态。因此这里，软件系统暴露的信息越详细，分析工具越强劲，内部状态就能推断地越准确与详细，那么该软件系统的可观测性就越好。\n不过，此时你可能有疑问：这些指标日志啥的不都是些的现成东西吗？那是不是可以说，本质上，传统意义的监控就是可观测性呢？这里的答案是否定的，且听我下文解释。\n与传统监控的区别 传统意义上的监控(Monitoring)是一种行为，这种行为分析系统的内部状态。而可观测性正如上文中描述的一样，是一种系统的性质，其他性质有健壮性、可测试性等\n在传统监控的场景下，我们收集、存储并分析指标数据。我们制作监控大盘并设置各种告警，当发生异常时，告警触发,收到告警后，我们根据告警的内容以及相关的监控图表，来推断出异常发生的原因，并由此做出相应的处理（或增加资源、或修复Bug）。\n但是这里有一个很大前提，就是告警策略必须预先设置，而如何设置又完全取决于经验与直觉。换句话说，通过监控我们只能检测一些已知的潜在风险，例如机器的负载、CPU使用率、磁盘使用率等。而对于一个未知或复杂的系统，当它发生异常时，我们往往只能束手无策，或者通过一些线索去猜测可能的原因并验证，如果猜错了那又得重复上述过程，非常的浪费时间。\n而在可观测性的场景下，系统中植入了各种各样的代码和工具，并提供了非常丰富的可观测的数据（metric, logs, traces等各种数据），通过这些数据并结合合适的工具，我们能够很快地排查出问题的根因所在。举个例子，同样是一个未知的系统，当发现某个接口很慢时，我们可以通过链路追踪工具找到瓶颈点，通过瓶颈点再分析当时的系统资源使用率，饱和度，负载情况以及应用日志等，从而很快地定位出根因（资源问题？代码问题？第三方服务问题？等等）\n流行的原因 近10年IT相关行业发生了天翻地覆的变化。IT技术也是日新月异，尤其是微服务架构、分布式以及云原生的高速发展，以及各种敏捷开发思想深入人心。\n如今的软件系统已经与10年前的大不相同了，复杂度、灵活度、变化度等都大幅提升。而由此带来的问题就是，系统稳定性保障变得越来越困难，尤其是问题根因的定位\n 例如，对于一些复杂问题，有时候花费数个月都无法定位，最后的选择往往都是推倒重来\n 因此单靠传统的监控已经无法满足当下软件系统的观测需求，传统监控只能解决那些\u0026quot;known unknown/known\u0026quot;类的问题，而无法应对\u0026quot;unknown unknown\u0026quot;类的问题，而这类问题在如今的架构下要多得多。\n Known unknown/known：指的是你熟悉的已知或未知的软件系统，这种系统可预测，因此我们可以预先设置各种告警\nUnknown unknown：指的是你不熟悉的未知系统，这种系统完全未知，只能通过可观测性工具来探测\n 三大支柱是可观测性吗 一说到可观测性，可能最先联想的就是“三大支柱(the three pillars)”，即logs, metrics以及traces（如下图所示）。很多人（包括我）经常以为它们就是所谓的可观测性，毕竟很多PAAS平台和厂商就是这么宣传的，但这并不完全正确。\n是的，没错，三大支柱确实是可观测性体系里不可或缺的条件。但这并不代表我暴露了这些数据，我的软件系统就具备了可观测性，同样也不能代表我收集分析了这些数据，我就实现了一个可观测性工具系统。\n首先，可观测性需要的数据并非只有这三者，它还可以是用户的反馈信息、系统profiling信息等各种统计、事件信息；其次暴露的数据的维度、基数以及数据间的关联度等等都会影响系统的可观测性；而一个可观测性工具的搭建除了收集这些统计、时间信息，还包括数据传输与处理，数据存储以及交互的易用性等，此外涉及到的数据隔离，容量规划，低成本且高性能等问题也是十分棘手的。\n不过，话虽如此，“三大支柱”虽不能等同于可观测性，但它们是你迈向可观测性的第一步:)\n总结 综上所述，如果你的应用非常简单，比如一个单体应用，那么传统监控也足够满足需求了。但是一旦切换为微服务架构，甚至完全云原生化的开发方式时，此时软件系统的复杂度就成指数级增加了，而此时可观测性就显得异常重要。\n相信你都经过，一个软件系统随着业务的发展会变得越来越复杂，到最后每个人都只能往上面堆功能，而对老代码甚至不敢改动一行，最终软件系统就会变成人们口中的“屎山”，而后面的人的唯一选择只能是推到重来。\n而如果可观测性一开始就在架构考虑中，那么无论我们的的系统变化多大，多复杂，我们都能对其了如指掌并快速定位问题根因，此外还能提前发现到系统架构的不合理之处并及时调整。\n参考   Observability Engineering\n  https://www.splunk.com/en_us/data-insider/what-is-observability.html\n  https://en.wikipedia.org/wiki/Observability\n  https://www.dynatrace.com/news/blog/what-is-observability-2/\n  ","title":"聊一聊可观测性"},{"content":"问个问题，如何优化一条SQL语句？我们首先想到的肯定是建索引。对于ClickHouse也不例外，尤其是稀疏主键索引（类似传统数据库中的主键索引）对性能的影响非常大。在下文中，我将结合例子对稀疏主键索引进行详细解读。\n 注：本文内容主要参考官方文档，如果有余力，强烈建议先行阅读\n 数据准备 这里，我将就我比较熟悉的时序数据进行举例。首先通过如下SQL建表：\n-- create dabase create database test; use test; -- create table CREATE TABLE cpu ( `hostname` String, `reginon` String, `datacenter` String, `timestamp` DateTime64(9,\u0026#39;Asia/Shanghai\u0026#39;) CODEC (DoubleDelta), `usage_user` Float64, `usage_system` Float64 ) ENGINE = MergeTree() PRIMARY KEY tuple(); optimize table cpu final ; 并将本地的样本数据导入：\ncat example/output.csv |clickhouse-client -d test -q \u0026#39;INSERT into cpu FORMAT CSV\u0026#39;    output.csv是时序数据样本，时间间隔为1秒，包含了从2022-01-01 08:00:00到2022-01-15 07:59:59一共1209600条记录\n  optimize table 会强制进行merge之类的操作，使其达到最终状态\n   查询某段时间范围内的CPU使用率 SQL如下:\nselect ts, avg(usage_user) from cpu where timestamp \u0026gt; \u0026#39;2022-01-15 06:59:59.000000000\u0026#39; and timestamp \u0026lt; \u0026#39;2022-01-15 07:59:59.000000000\u0026#39; group by toStartOfMinute(timestamp) as ts order by ts; 统计结果如下：\n60 rows in set. Elapsed: 0.025 sec. Processed 1.21 million rows, 9.72 MB (48.68 million rows/s., 391.19 MB/s.)  注意：clickhouse客户端对每次查询会给出简要的性能数据，便于用户进行简单分析\n 可以看到，尽管我们只查询了一个小时范围内的数据，但是依然扫描121万行数据，也就是进行了一次全表扫描！\n显然，这样的是不够的，我们需要建立合理的稀疏索引，而它将显著提高查询性能。\n稀疏主键索引 稀疏主键索引(Sparse Primary Indexes)，以下简称稀疏索引。其功能上类似于MySQL中的主键索引，不过实现原理上是截然不同的。\n长话短说，如若建立类似于B+树那种面向具体行的索引，在面对大数据场景时，必将占用大量内存和磁盘并且严重影响写入性能。此外，基于ClickHouse的实际使用场景考虑，也无需精确定位到每一行。\n因此，其总体设计上，ClickHouse将数据块按组（粒度）划分，并通过下标（mark）标记。这种设计使得索引的内存占用足够小，同时仍能显著提高查询性能，尤其是OLAP场景下的大数据量的范围查询和数据分析。\n配置Primary Key 稀疏索引可通过PRIMARY KEY语法指定，接下来让我们通过它来优化示例中的cpu表吧:\n-- create table with timestamp as primary key CREATE TABLE cpu_ts ( `hostname` String, `reginon` String, `datacenter` String, `timestamp` DateTime64(9,\u0026#39;Asia/Shanghai\u0026#39;) CODEC (DoubleDelta), `usage_user` Float64, `usage_system` Float64 ) ENGINE = MergeTree() PRIMARY KEY (timestamp) ORDER BY (timestamp); -- insert data from cpu insert into cpu_ts select * from cpu; optimize table cpu_ts final ; 执行相同的分析SQL：\nselect ts, avg(usage_user) from cpu_ts where timestamp \u0026gt; \u0026#39;2022-01-15 06:59:59.000000000\u0026#39; and timestamp \u0026lt; \u0026#39;2022-01-15 07:59:59.000000000\u0026#39; group by toStartOfMinute(timestamp) as ts order by ts; 输出如下：\n60 rows in set. Elapsed: 0.004 sec. Processed 12.29 thousand rows, 196.58 KB (3.21 million rows/s., 51.31 MB/s.) 可以看到同样的结果，仅扫描5千多行! 下面，让我们通过EXPALIN来分析：\nEXPLAIN indexes = 1 select ts, avg(usage_user) from cpu_ts where timestamp \u0026gt; \u0026#39;2022-01-15 06:59:59.000000000\u0026#39; and timestamp \u0026lt; \u0026#39;2022-01-15 07:59:59.000000000\u0026#39; group by toStartOfMinute(timestamp) as ts order by ts; 输出如下：\n┌─explain────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ Expression (Projection) │ │ MergingSorted (Merge sorted streams for ORDER BY) │ │ MergeSorting (Merge sorted blocks for ORDER BY) │ │ PartialSorting (Sort each block for ORDER BY) │ │ Expression (Before ORDER BY) │ │ Aggregating │ │ Expression (Before GROUP BY) │ │ Filter (WHERE) │ │ SettingQuotaAndLimits (Set limits and quota after reading from storage) │ │ ReadFromMergeTree │ │ Indexes: │ │ PrimaryKey │ │ Keys: │ │ timestamp │ │ Condition: and((timestamp in (-inf, \u0026#39;1642204799.000000000\u0026#39;)), (timestamp in (\u0026#39;1642201199.000000000\u0026#39;, +inf))) │ │ Parts: 1/1 │ │ Granules: 1/148 │ └────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘ 17 rows in set. Elapsed: 0.003 sec. 可以看到，借助主键索引，我们仅仅扫描了一个粒度的数据，优化效果十分明显 ！\n实现原理 数据排布 我们不妨从实际磁盘上的数据文件来对ClickHouse有个总体上的理解。首先，让我们来看下cpu_ts表的某个partall_1_1_0/在磁盘上的存储结构：\n all_1_1_0其结构表示为： {分区}_{最小block数}_{最大block数}_{表示经历了几次merge}\n -rw-r----- 1 clickhouse clickhouse 589 Jun 26 05:42 checksums.txt -rw-r----- 1 clickhouse clickhouse 179 Jun 26 05:42 columns.txt -rw-r----- 1 clickhouse clickhouse 7 Jun 26 05:42 count.txt -rw-r----- 1 clickhouse clickhouse 55K Jun 26 05:42 datacenter.bin -rw-r----- 1 clickhouse clickhouse 3.4K Jun 26 05:42 datacenter.mrk2 -rw-r----- 1 clickhouse clickhouse 10 Jun 26 05:42 default_compression_codec.txt -rw-r----- 1 clickhouse clickhouse 34K Jun 26 05:42 hostname.bin -rw-r----- 1 clickhouse clickhouse 3.4K Jun 26 05:42 hostname.mrk2 -rw-r----- 1 clickhouse clickhouse 1.2K Jun 26 05:42 primary.idx -rw-r----- 1 clickhouse clickhouse 51K Jun 26 05:42 reginon.bin -rw-r----- 1 clickhouse clickhouse 3.4K Jun 26 05:42 reginon.mrk2 -rw-r----- 1 clickhouse clickhouse 147K Jun 26 05:42 timestamp.bin -rw-r----- 1 clickhouse clickhouse 3.4K Jun 26 05:42 timestamp.mrk2 -rw-r----- 1 clickhouse clickhouse 2.5M Jun 26 05:42 usage_system.bin -rw-r----- 1 clickhouse clickhouse 3.4K Jun 26 05:42 usage_system.mrk2 -rw-r----- 1 clickhouse clickhouse 2.5M Jun 26 05:42 usage_user.bin -rw-r----- 1 clickhouse clickhouse 3.4K Jun 26 05:42 usage_user.mrk2 其中主要文件如下:\n *.bin：每个表中的列上的值压缩后的数据文件 column.txt：此表中所有列以及每列的类型 default_compression_codec.txt：数据文件默认的压缩算法 primary.idx：数据索引，后文详解 *.mrk2：数据标记，后文详解 count.txt：此part中数据的行数   注意：通过指定的PRIMARY KEY，其数据行会以timestamp升序排列并生成主键索引，对于timestamp相同的行按照ORDER BY中的hostname进行(若PRIMARY KEY和ORDER BY分别指定，则PRIMARY KEY必须是ORDER BY的前缀)。\n 索引建立 上文提到ClickHouse会对数据进行分组，其中某一列内的值自然也是按照组(粒度，默认8192行)进行划分的。粒度，可以简单理解为ClickHouse中一组不可划分的最小数据单元。\n当处理数据时，ClickHouse会读取整个粒度内的所有数据，而不是一行一行地读取（批处理的思想无处不在哈）。因此对于cpu_ts表内的数据，会被划分为148个组(ceil(1210000/8192=148)，其整体数据排布如下所示：\n从上图可以看出，表内数据被按照每8192行划分为148个粒度，每个组内取PRIMARY KEY中指定列的最小数据作为数据，并通过标记给其编号，存储在primary.idx中（可以将其理解为一个数组，标记就是下标）。我们使用od命令od -l -j 0 -N 32 primary.idx将primary.idx打印出来：\n0000000 1640995200000000000 1641003392000000000 0000020 1641011584000000000 1641019776000000000 # 粒度数组：[1640995200000000000, 1641003392000000000, 1641011584000000000, 1641019776000000000]  (1642151554000000000-1642143362000000000)/1000000000 = 8192\n 当我们根据timestamp字段进行过滤时，ClickHouse会通过二分搜索算法对primary.txt中的timestamp列进行搜索，找到对应的组标记，从而实现快速定位。例如对于示例查询语句，ClickHouse定位到的标记是148，那么又要如何通过标记反向找到实际的数据块呢？\n数据定位 这里，ClickHouse是通过引入{column_name}.mrk文件来解决的，mrk文件存储了粒度编号与压缩前后数据的偏移量(offset)，其大致结构如下图所示：\n使用od命令od -l -j 0 -N 24 timestamp.mrk2输出如下：\n0000000 0 0 0000020 8192 1071 0000040 0 8192  # (0, 0, 8192)为一组，表示压缩后偏移量为0，压缩前偏移量为0，粒度内一共8192行 # (1071, 0, 8192)为一组，表示压缩后偏移量为1071， 压缩前偏移量为0，粒度内一共8192行 在前文中，我们已经通过primary.idx已经拿到了具体的粒度编号(mark)，接着我们通过编号在{column}.mrk中找到对应的数据压缩前后的偏移量。然后通过以下2个步骤将数据发送给分析引擎：\n 通过压缩后的偏移量定位到数据文件(*.bin)中的数据块并解压后加载到内存中 根据压缩前的偏移量定位到内存中相关未压缩的数据块，然后将其发送到分析引擎  总结 综上所述，ClickHouse的稀疏索引是综合权衡之下的产物，尽管其使用了一种看起来比较粗粒度的索引机制，但依然能获得达到相当客观的性能提升。毕竟默认8192行的粒度，对于动辄上亿级别的OLAP场景来说已经算是比较细粒度的了，同时得益于ClickHouse强大的并行计算与分析能力，其查询的性能需求是能够满足的。\n不过实际的使用场景中，由于PRIMARY KEY一旦定义就没法更改了，而实际的查询方式又往往是变化无常的。因此单靠稀疏索引有时无法满足实际需求。\nClickHouse为此额外提供了两种方案：一种是通过定义新的PRIMARY KEY，并通过创建新表或物化表之类来重建；而另外一种则是类似传统二级索引的机制叫做跳数索引来处理，这些我将在后序文章中进行介绍:)\n参考  https://github.com/timescale/tsbs https://clickhouse.com/docs/en/guides/improving-query-performance https://zhuanlan.zhihu.com/p/397411559 https://stackoverflow.com/questions/65198241/whats-the-process-of-clickhouse-primary-index  ","permalink":"https://erenming.github.io/posts/clickhouse-sparse-index/","summary":"问个问题，如何优化一条SQL语句？我们首先想到的肯定是建索引。对于ClickHouse也不例外，尤其是稀疏主键索引（类似传统数据库中的主键索引）对性能的影响非常大。在下文中，我将结合例子对稀疏主键索引进行详细解读。\n 注：本文内容主要参考官方文档，如果有余力，强烈建议先行阅读\n 数据准备 这里，我将就我比较熟悉的时序数据进行举例。首先通过如下SQL建表：\n-- create dabase create database test; use test; -- create table CREATE TABLE cpu ( `hostname` String, `reginon` String, `datacenter` String, `timestamp` DateTime64(9,\u0026#39;Asia/Shanghai\u0026#39;) CODEC (DoubleDelta), `usage_user` Float64, `usage_system` Float64 ) ENGINE = MergeTree() PRIMARY KEY tuple(); optimize table cpu final ; 并将本地的样本数据导入：\ncat example/output.csv |clickhouse-client -d test -q \u0026#39;INSERT into cpu FORMAT CSV\u0026#39;    output.csv是时序数据样本，时间间隔为1秒，包含了从2022-01-01 08:00:00到2022-01-15 07:59:59一共1209600条记录\n  optimize table 会强制进行merge之类的操作，使其达到最终状态","title":"ClickHouse稀疏索引原理解读"},{"content":"记录一下使用Hugo生成静态博客，并通过githubPages自动化部署的过程。\n这里，我的目标是：\n 使用blog-source作为原始的内容仓库，\u0026lt;your-name\u0026gt;.github.io作为实际的githubPages仓库 通过github Action将两者串联起来，原始内容提交变更时，自动触发内容生成并发布  这样的好处是，可以将blog-source作为私有仓库，并能直接以\u0026lt;your-name\u0026gt;.github.io作为URL。且通过github action实现CICD，解放双手实现自动化。这里我画了一张图，便于理解：\nHugo 安装Hugo，然后初始化\n# macOS install hugo brew install hugo  # create site project hugo new site blog-source 选择你中意的主题并安装\ncd blog-source git init  # add paperMod as theme git submodule add https://github.com/adityatelange/hugo-PaperMod themes/paperMod 添加文章并启动demo\nhugo new posts/my-first-post.md # start demo for preview hugo server -D 创建一个额外的仓库，这里我创建一个名为blog-source的仓库并作为刚才创建的blog-source的远端仓库\ncd blog-source git init git remote add origin \u0026lt;your-remove-git\u0026gt; GithubPages 创建一个githubPages仓库，名称必须是\u0026lt;your-name\u0026gt;.github.io。DOC\nConnection 创建sshKey: ssh-keygen -t rsa -b 4096 -C \u0026quot;$(git config user.email)\u0026quot; -f gh-pages -N \u0026quot;\u0026quot;\n将私钥gh-pages内容复制，并在blog-source仓库的Settings-\u0026gt;Secrets-\u0026gt;Actions创建secret变量\n将公钥gh-pages.pub内容复制，并作为\u0026lt;your-name\u0026gt;.github.io的Deploy Key，记得勾选读写权限\n创建github workflow文件.github/workflows/gh-pages.yml，其内容如下所示：\nname: github pages  on:  push:  branches:  - master  # Set a branch to deploy, my branch is master  pull_request:  jobs:  deploy:  runs-on: ubuntu-20.04  steps:  - uses: actions/checkout@v2  with:  submodules: true # Fetch Hugo themes (true OR recursive)  fetch-depth: 0 # Fetch all history for .GitInfo and .Lastmod   - name: Setup Hugo  uses: peaceiris/actions-hugo@v2  with:  hugo-version: \u0026#39;0.99.1\u0026#39;  # extended: true   - name: Build  run: hugo --minify   - name: Deploy  uses: peaceiris/actions-gh-pages@v3  with:  external_repository: \u0026lt;your-name\u0026gt;/\u0026lt;your-name\u0026gt;.github.io  publish_branch: master  # the secret key  deploy_key: ${{ secrets.ACTIONS_DEPLOY_KEY }}  publish_dir: ./public 完成后，在blog-source仓库提交代码并push即可触发workflow，可在仓库的Actions功能项下查看运行情况。\n若无意外，稍等片刻(估计是因为Github同步并非完全实时)，即可通过\u0026lt;your-name\u0026gt;/\u0026lt;your-name\u0026gt;.github.io访问博客了\n总结 综上所述，使用hugo生成静态网站，创建githubPages项目其实并不难，主要难点在于如何通过githubAction将两者链接起来，实现CICD。遇到问题，建议多多翻阅官方文档，一定是能解决的。\n参考  https://github.com/peaceiris/actions-gh-pages https://docs.github.com/en/pages/getting-started-with-github-pages/about-github-pages https://gohugo.io/hosting-and-deployment/hosting-on-github/  ","permalink":"https://erenming.github.io/posts/hugo-with-githubpages/","summary":"记录一下使用Hugo生成静态博客，并通过githubPages自动化部署的过程。\n这里，我的目标是：\n 使用blog-source作为原始的内容仓库，\u0026lt;your-name\u0026gt;.github.io作为实际的githubPages仓库 通过github Action将两者串联起来，原始内容提交变更时，自动触发内容生成并发布  这样的好处是，可以将blog-source作为私有仓库，并能直接以\u0026lt;your-name\u0026gt;.github.io作为URL。且通过github action实现CICD，解放双手实现自动化。这里我画了一张图，便于理解：\nHugo 安装Hugo，然后初始化\n# macOS install hugo brew install hugo  # create site project hugo new site blog-source 选择你中意的主题并安装\ncd blog-source git init  # add paperMod as theme git submodule add https://github.com/adityatelange/hugo-PaperMod themes/paperMod 添加文章并启动demo\nhugo new posts/my-first-post.md # start demo for preview hugo server -D 创建一个额外的仓库，这里我创建一个名为blog-source的仓库并作为刚才创建的blog-source的远端仓库\ncd blog-source git init git remote add origin \u0026lt;your-remove-git\u0026gt; GithubPages 创建一个githubPages仓库，名称必须是\u0026lt;your-name\u0026gt;.github.io。DOC\nConnection 创建sshKey: ssh-keygen -t rsa -b 4096 -C \u0026quot;$(git config user.","title":"使用Hugo部署GithubPages"},{"content":"不知为何，写博客总是断断续续，距离上次更新已过去快一年了，直至如今面试碰壁方才后悔莫及。\n事实上，写博客对于对于知识的小伙理解非常有好处，毕竟要让别人听得懂，首先自己得更懂才行嘛。因为，通常学习一项新知识，通常需要理论学习-实践-总结输出三个阶段，而我往往只完成了第一阶段便草草了事，无法对知识有更深入的理解。因此，我打算重启我的博客之路，持续学习持续输出。\n之前博客用过Hexo，也用过博客园等等，最近看到hugo的paperMod主题非常讨喜，因此打算彻底切换到hugo+paperMod，也算起个好头吧。之前的文章也会从博客园迁移到hugo上，不过后续两边也会尽量同步更新。\n然后，也打算支持中英文双语，主要是为了提升自己的英文水平，以便能和世界上的程序员更好地交流。不过目前主打还是中文，母语写起来还是方便点，英文会挑选文章进行编写翻译，同时英文文章也会同步发布在Medium上。\n此外，为了降低断更的概率、提高文章质量，我在这里给自己立一个flag，即每月至少更新一篇文章，文章长度适宜、做到通俗易懂、绝不模棱两可故作高深\n","permalink":"https://erenming.github.io/posts/restart-my-blog/","summary":"不知为何，写博客总是断断续续，距离上次更新已过去快一年了，直至如今面试碰壁方才后悔莫及。\n事实上，写博客对于对于知识的小伙理解非常有好处，毕竟要让别人听得懂，首先自己得更懂才行嘛。因为，通常学习一项新知识，通常需要理论学习-实践-总结输出三个阶段，而我往往只完成了第一阶段便草草了事，无法对知识有更深入的理解。因此，我打算重启我的博客之路，持续学习持续输出。\n之前博客用过Hexo，也用过博客园等等，最近看到hugo的paperMod主题非常讨喜，因此打算彻底切换到hugo+paperMod，也算起个好头吧。之前的文章也会从博客园迁移到hugo上，不过后续两边也会尽量同步更新。\n然后，也打算支持中英文双语，主要是为了提升自己的英文水平，以便能和世界上的程序员更好地交流。不过目前主打还是中文，母语写起来还是方便点，英文会挑选文章进行编写翻译，同时英文文章也会同步发布在Medium上。\n此外，为了降低断更的概率、提高文章质量，我在这里给自己立一个flag，即每月至少更新一篇文章，文章长度适宜、做到通俗易懂、绝不模棱两可故作高深","title":"重启博客之路"},{"content":"为什么需要内存分配器？ 总说周知，内存作为一种相对稀缺的资源，在操作系统中以虚拟内存的形式来作为一种内存抽象提供给进程，这里可以简单地把它看做一个连续的地址集合{0, 1, 2, ..., M}，由栈空间、堆空间、代码片、数据片等地址空间段组合而成，如下图所示(出自CS:APP3e, Bryant and O\u0026rsquo;Hallaron的第9章第9节)\n这里我们重点关注Heap（堆），堆是一块动态的虚拟内存地址空间。在C语言中，我们通常使用malloc来申请内存以及使用free来释放内存，也许你想问，这样不就足够了吗？但是，这种手动的内存管理会带来很多问题，比如：\n 给程序员带来额外的心智负担，必须得及时释放掉不再使用的内存空间，否则就很容易出现内存泄露 随着内存的不断申请与释放，会产生大量的内存碎片，这将大大降低内存的利用率  因此，正确高效地管理内存空间是非常有必要的，常见的技术实现有Sequential allocation, Free-List allocation等。那么，在Go中，内存是如何被管理的呢？\n 注：此为Go1.13.6的实现逻辑，随版本更替某些细节会有些许不同\n 实现原理 Go的内存分配器是基于TCMalloc设计的，因此我建议你先行查阅，这将有利于理解接下来的内容。\n大量工程经验证明，程序中的小对象占了绝大部分，且生命周期都较为短暂。因此，Go将内存划分为各种类别(Class)，并各自形成Free-List。相较于单一的Free-List分配器，分类后主要有以下优点：\n  其一方面减少不必要的搜索时间，因为对象只需要在其所属类别的空闲链表中搜索即可\n  另一方面减少了内存碎片化，同一类别的空闲链表，每个对象分配的空间都是一样大小(不足则补齐)，因此该链表除非无空闲空间，否则总能分配空间，避免了内存碎片\n  那么，Go内存分配器具体是如何实现的呢？接下来，我将以自顶向下的方式，从宏观到微观，层层拨开她的神秘面纱。\n数据结构 首先，介绍Go内存分配中相关的数据结构。其总体概览图如下所示：\nheapArena 在操作系统中，我们一般把堆看做是一块连续的虚拟内存空间。\nGo将其划分为数个相同大小的连续空间块，称之arena，其中，heapArena则作为arena空间的管理单元，其结构如下所示：\ntype heapArena struct {  bitmap [heapArenaBitmapBytes]byte  spans [pagesPerArena]*mspan  ... }  bitmap: 表示arena区域中的哪些地址保存了对象，哪些地址保存了指针 spans: 表示arena区域中的哪些操作系统页(8K)属于哪些mspan  mheap 然后，则是核心角色mheap了，它是Go内存管理中的核心数据结构，作为全局唯一变量，其结构如下所示：\ntype mheap struct { \tfree mTreap  ...  allspans []*mspan  ...  arenas [1 \u0026lt;\u0026lt; arenaL1Bits]*[1 \u0026lt;\u0026lt; arenaL2Bits]*heapArena  ...  central [numSpanClasses]struct { \tmcentral mcentral \tpad [cpu.CacheLinePadSize - unsafe.Sizeof(mcentral{})%cpu.CacheLinePadSize]byte \t} }  free: 使用树堆的结构来保存各种类别的空闲mspan allspans: 用以记录了分配过了的mspan arenas: 表示其覆盖的所有arena区域，通过虚拟内存地址计算得到下标索引 central: 表示其覆盖的所有mcentral，一共134个，对应67个类别  mcentral 而mcentral充当mspan的中心管理员，负责管理某一类别的mspan，其结构如下：\ntype mcentral struct { \tlock mutex \tspanclass spanClass \tnonempty mSpanList \tempty mSpanList }  lock: 全局互斥锁，因为多个线程会并发请求 spanclass：mspan类别 nonempty：mspan的双端链表，且其中至少有一个mspan包含空闲对象 empty：mspan的双端链表，但不确定其中的mspan是否包含空闲对象  mcache mcache充当mspan的线程本地缓存角色，其与线程处理器(P)一一绑定。\n这样呢，当mcache有空闲mspan时，则无需向mcentral申请，因此可以避免诸多不必要的锁消耗。结构如下所示：\ntype mcache struct {  ...  alloc [numSpanClasses]*mspan  ... }  alloc: 表示各个类别的mspan  mspan mspan作为虚拟内存的实际管理单元，管理着一片内存空间(npages个页)，其结构如下所示：\ntype mspan struct { \tnext *mspan // 指向下一个mspan \tprev *mspan // 指向前一个mspan  ... \tnpages uintptr  freeindex uintptr  nelems uintptr // 总对象个数  ...  allocBits *gcBits \tgcmarkBits *gcBits }  next指针指向下一个mspan，prev指针指向前一个mspan，因此各个mspan彼此之间形成一个双端链表，并被runtime.mSpanList作为链表头。 npages：mspan所管理的页的数量 freeindex：空闲对象的起始位置，如果freeindex等于nelems时，则代表此mspan无空闲对象可分配了 allocBits：标记哪些元素已分配，哪些未分配。与freeindex结合，可跳过已分配的对象 gcmarkBits：标记哪些对象存活，每次GC结束时，将其设为allocBits  通过上述对Go内存管理中各个关键数据结构的介绍，想必现在，我们已经对其有了一个大概的轮廓。接下来，让我们继续探究，看看Go具体是如何利用这些数据结构来实现高效的内存分配算法\n算法 分配内存 内存分配算法，其主要函数为runtime.mallocgc，其基本步骤简述如下：\n 判断待分配对象的大小 若对象小于maxTinySize（16B），且不为指针，则执行微对象分配算法 若对象小于maxSmallSize（32KB），则执行小对象分配算法 否则，则执行大对象分配算法  在微对象以及小对象分配过程中，如果span中找不到足够的空闲空间，Go会触发层级的内存分配申请策略。其基本步骤如下：\n 先从mcache寻找对应类别的span，若有空闲对象，则成功返回 若无，则向mcentral申请，分别从nonempty和empty中寻找匹配的span，若找到，则成功返回 若还未找到，则继续向mheap申请，从mheap.free中寻找，若找到，则成功返回 若未找到，则需扩容，从关联的arena中申请，若关联的arena中空间也不足，则向OS申请额外的arena 扩容完毕后，继续从mheap.free中寻找，若仍未找到，则抛出错误  学到了什么  本地线程缓存，提高性能：通过mcache缓存小对象的span，并优先在mcache中分配，降低锁竞争 无处不在的BitMap应用场景：通过二进制位来映射对象，例如mspan.allocBits用以表示对象是否分配 多级分配策略：自底向上，性能损耗：低-\u0026gt;高，频率：高-\u0026gt;低，能有效提高性能，思想上类似CPU中的多级缓存  总结 本文主要介绍了Go内存分配中的一些重要组件以及分配算法。可以看到，其主要思想还是基于TCMalloc的策略，将对象根据大小分类，并使用不同的分配策略。此外，还采用逐层的内存申请策略，大大提高内存分配的性能。\n参考  https://google.github.io/tcmalloc/ http://goog-perftools.sourceforge.net/doc/tcmalloc.html https://medium.com/@ankur_anand/a-visual-guide-to-golang-memory-allocator-from-ground-up-e132258453ed https://www.cnblogs.com/zkweb/p/7880099.html https://www.cnblogs.com/luozhiyun/p/14349331.html https://draveness.me/golang/docs/part3-runtime/ch07-memory/golang-memory-allocator/  ","permalink":"https://erenming.github.io/posts/memory-allocator-in-go/","summary":"为什么需要内存分配器？ 总说周知，内存作为一种相对稀缺的资源，在操作系统中以虚拟内存的形式来作为一种内存抽象提供给进程，这里可以简单地把它看做一个连续的地址集合{0, 1, 2, ..., M}，由栈空间、堆空间、代码片、数据片等地址空间段组合而成，如下图所示(出自CS:APP3e, Bryant and O\u0026rsquo;Hallaron的第9章第9节)\n这里我们重点关注Heap（堆），堆是一块动态的虚拟内存地址空间。在C语言中，我们通常使用malloc来申请内存以及使用free来释放内存，也许你想问，这样不就足够了吗？但是，这种手动的内存管理会带来很多问题，比如：\n 给程序员带来额外的心智负担，必须得及时释放掉不再使用的内存空间，否则就很容易出现内存泄露 随着内存的不断申请与释放，会产生大量的内存碎片，这将大大降低内存的利用率  因此，正确高效地管理内存空间是非常有必要的，常见的技术实现有Sequential allocation, Free-List allocation等。那么，在Go中，内存是如何被管理的呢？\n 注：此为Go1.13.6的实现逻辑，随版本更替某些细节会有些许不同\n 实现原理 Go的内存分配器是基于TCMalloc设计的，因此我建议你先行查阅，这将有利于理解接下来的内容。\n大量工程经验证明，程序中的小对象占了绝大部分，且生命周期都较为短暂。因此，Go将内存划分为各种类别(Class)，并各自形成Free-List。相较于单一的Free-List分配器，分类后主要有以下优点：\n  其一方面减少不必要的搜索时间，因为对象只需要在其所属类别的空闲链表中搜索即可\n  另一方面减少了内存碎片化，同一类别的空闲链表，每个对象分配的空间都是一样大小(不足则补齐)，因此该链表除非无空闲空间，否则总能分配空间，避免了内存碎片\n  那么，Go内存分配器具体是如何实现的呢？接下来，我将以自顶向下的方式，从宏观到微观，层层拨开她的神秘面纱。\n数据结构 首先，介绍Go内存分配中相关的数据结构。其总体概览图如下所示：\nheapArena 在操作系统中，我们一般把堆看做是一块连续的虚拟内存空间。\nGo将其划分为数个相同大小的连续空间块，称之arena，其中，heapArena则作为arena空间的管理单元，其结构如下所示：\ntype heapArena struct {  bitmap [heapArenaBitmapBytes]byte  spans [pagesPerArena]*mspan  ... }  bitmap: 表示arena区域中的哪些地址保存了对象，哪些地址保存了指针 spans: 表示arena区域中的哪些操作系统页(8K)属于哪些mspan  mheap 然后，则是核心角色mheap了，它是Go内存管理中的核心数据结构，作为全局唯一变量，其结构如下所示：\ntype mheap struct { \tfree mTreap  ...  allspans []*mspan  .","title":"浅析Go内存分配器的实现"},{"content":"最近做了许多有关Go内存优化的工作，总结了一些定位、调优方面的套路和经验，于是，想通过这篇文章与大家分享讨论。\n发现问题 性能优化领域有一条总所周知的铁律，即：不要过早地优化。编写一个程序，首先应该保证其功能的正确性，以及诸如设计是否合理、需求等是否满足，过早地优化只会引入不必要的复杂度以及设计不合理等各种问题。\n那么何时才能开始优化呢？一句话，问题出现时。诸如程序出现频繁OOM，CPU使用率异常偏高等情况。如今，在这微服务盛行的时代，公司内部都会拥有一套或简单或复杂的监控系统，当系统给你发出相关告警时，你就要开始重视起来了。\n问题定位 1. 查看内存曲线 首先，当程序发生OOM时，首先应该查看程序的内存使用量曲线，可以通过现有监控系统查看，或者prometheus之类的开源工具。\n曲线一般都是呈上升趋势，比如goroutine泄露的曲线一般是使用量缓慢上升直至OOM，而内存分配不合理往往时在高负载时快速攀升以致OOM。\n2. 问题复现 这块是可选项，但是最好能保证复现。如果能在本地或debug环境复现问题，这将非常有利于我们反复进行测试和验证。\n3. 使用pprof定位 Go官方工具提供了pporf来专门用以性能问题定位，首先得在程序中开启pprof收集功能，这里假定问题程序已开启pprof。(对这块不够了解的同学，建议通过这两篇文章(1, 2)学习下pprof工具的基本用法)\n接下来，我们复现问题场景，并及时获取heap和groutine的采样信息。\n 获取heap信息: curl http://loalhost:6060/debug/pprof/heap -o h1.out 获取groutine信息：curl http://loalhost:6060/debug/pprof/goroutine -o g1.out  这里你可能想问，这样就够了吗？\n当然不是，只获取一份样本信息是不够的。内存使用量是不断变化的(通常是上升)，因此我们需要的也是期间heap、gourtine信息的变化信息，而非瞬时值。一般来说，我们需要一份正常情况下的样本信息，一份或多份内存升高期间的样本信息。\n数据收集完毕后，我们按照如下3个方面来排查定位。\n排查goroutine泄露 使用命令go tool pprof --base g1.out g2.out ，比较goroutine信息来判断是否有goroutine激增的情况。\n进入交互界面后，输入top命令，查看期间goroutine的变化。\n同时可执行go tool pprof --base g2.out g3.out来验证。我之前写了的一篇实战文章，记录了goroutine泄露的排查过程。\n排查内存使用量 使用命令go tool pprof --base h1.out h2.out，比较当前堆内存的使用量信息来判断内存使用量。\n进入交互界面后，输入top命令，查看期间堆内存使用量的变化。\n排查内存分配量 当上述排查方向都没发现问题时，那就要查看期间是否有大量的内存申请了，以至于GC都来不及回收。使用命令go tool pprof --alloc_space --base h1.out h2.out，通过比较前后内存分配量来判断是否有分配不合理的现象。\n进入交互界面后，输入top命令，查看期间堆内存分配量的变化。\n一般来说，通过上述3个方面的排查，我们基本就能定位出究竟是哪方面的问题导致内存激增了。我们可以通过web命令，更为直观地查看问题函数(方法)的完整调用链。\n问题优化 定位到问题根因后，接下来就是优化阶段了。这个阶段需要对Go本身足够熟悉，还得对问题程序的业务逻辑有所了解。\n我梳理了一些常见的优化手段，仅供参考。实际场景还是得实际分析。\ngoroutine泄露 这种问题还是比较好修复的，需要显式地保证goroutine能正确退出，而非以一些自以为的假设来保证。例如，通过传递context.Context对象来显式退出\ngo func(ctx context.Context) {  for {  select {  case \u0026lt;-ctx.Done():  default:  }  ...  } }(ctx) 对象复用 在一些热点代码处，我们应该避免每次调用都申请新的内存，因为在极端情况下，内存分配速度可能会超过GC的速度，从而导致内存激增。这种情况下，我们可以采取复用对象的方式，例如我们可以使用sync.Pool来复用对象\nvar pool = sync.Pool{New: func() interface{} { return make([]byte, 4096) }}  func fn() { \tbuf := pool.Get().([]byte) // takes from pool or calls New \t// do work \tpool.Put(buf) // returns buf to the pool } 避免[]byte和string转换 在Go中，使用string()或[]byte()来实现[]byte和string的类型转换，会额外申请一块内存来复制。我们可以通过一些技巧来避免复制，例如*(*[]byte)(unsafe.Pointer(\u0026amp;s))来实现string转[]byte\n除此之外，还有很多优化方法，可以看看dave cheney大神的这篇文章，真得写得非常好。\n优化验证 最后一步，我们需要验证优化的结果，毕竟你至少得说服自己，你的优化是的确有成效的。\n除了通过复现测试来验证有效性外的，还可以编写Benchmark测试用例来比较优化前后的内存分配情况（在Benchmark测试用例中加入一行b.ReportAllocs()，即可得到内存分配量信息）\n总结 性能调优是一项必备但是较为困难的技能，不仅需要熟悉语言、操作系统等基本知识，还需要一定的经验积累。\n本文介绍了针对Go程序内存问题的发现、定位、优化以及验证，希望能对你排查内存问题有所帮助（还有某些情况未能没考虑到，欢迎评论区参与讨论）。\n参考  https://dave.cheney.net/high-performance-go-workshop/dotgo-paris.html https://golang.org/pkg/net/http/pprof/ https://www.freecodecamp.org/news/how-i-investigated-memory-leaks-in-go-using-pprof-on-a-large-codebase-4bec4325e192/  ","permalink":"https://erenming.github.io/posts/memory-optimize-best-practice-for-golang/","summary":"最近做了许多有关Go内存优化的工作，总结了一些定位、调优方面的套路和经验，于是，想通过这篇文章与大家分享讨论。\n发现问题 性能优化领域有一条总所周知的铁律，即：不要过早地优化。编写一个程序，首先应该保证其功能的正确性，以及诸如设计是否合理、需求等是否满足，过早地优化只会引入不必要的复杂度以及设计不合理等各种问题。\n那么何时才能开始优化呢？一句话，问题出现时。诸如程序出现频繁OOM，CPU使用率异常偏高等情况。如今，在这微服务盛行的时代，公司内部都会拥有一套或简单或复杂的监控系统，当系统给你发出相关告警时，你就要开始重视起来了。\n问题定位 1. 查看内存曲线 首先，当程序发生OOM时，首先应该查看程序的内存使用量曲线，可以通过现有监控系统查看，或者prometheus之类的开源工具。\n曲线一般都是呈上升趋势，比如goroutine泄露的曲线一般是使用量缓慢上升直至OOM，而内存分配不合理往往时在高负载时快速攀升以致OOM。\n2. 问题复现 这块是可选项，但是最好能保证复现。如果能在本地或debug环境复现问题，这将非常有利于我们反复进行测试和验证。\n3. 使用pprof定位 Go官方工具提供了pporf来专门用以性能问题定位，首先得在程序中开启pprof收集功能，这里假定问题程序已开启pprof。(对这块不够了解的同学，建议通过这两篇文章(1, 2)学习下pprof工具的基本用法)\n接下来，我们复现问题场景，并及时获取heap和groutine的采样信息。\n 获取heap信息: curl http://loalhost:6060/debug/pprof/heap -o h1.out 获取groutine信息：curl http://loalhost:6060/debug/pprof/goroutine -o g1.out  这里你可能想问，这样就够了吗？\n当然不是，只获取一份样本信息是不够的。内存使用量是不断变化的(通常是上升)，因此我们需要的也是期间heap、gourtine信息的变化信息，而非瞬时值。一般来说，我们需要一份正常情况下的样本信息，一份或多份内存升高期间的样本信息。\n数据收集完毕后，我们按照如下3个方面来排查定位。\n排查goroutine泄露 使用命令go tool pprof --base g1.out g2.out ，比较goroutine信息来判断是否有goroutine激增的情况。\n进入交互界面后，输入top命令，查看期间goroutine的变化。\n同时可执行go tool pprof --base g2.out g3.out来验证。我之前写了的一篇实战文章，记录了goroutine泄露的排查过程。\n排查内存使用量 使用命令go tool pprof --base h1.out h2.out，比较当前堆内存的使用量信息来判断内存使用量。\n进入交互界面后，输入top命令，查看期间堆内存使用量的变化。\n排查内存分配量 当上述排查方向都没发现问题时，那就要查看期间是否有大量的内存申请了，以至于GC都来不及回收。使用命令go tool pprof --alloc_space --base h1.out h2.out，通过比较前后内存分配量来判断是否有分配不合理的现象。\n进入交互界面后，输入top命令，查看期间堆内存分配量的变化。\n一般来说，通过上述3个方面的排查，我们基本就能定位出究竟是哪方面的问题导致内存激增了。我们可以通过web命令，更为直观地查看问题函数(方法)的完整调用链。\n问题优化 定位到问题根因后，接下来就是优化阶段了。这个阶段需要对Go本身足够熟悉，还得对问题程序的业务逻辑有所了解。\n我梳理了一些常见的优化手段，仅供参考。实际场景还是得实际分析。\ngoroutine泄露 这种问题还是比较好修复的，需要显式地保证goroutine能正确退出，而非以一些自以为的假设来保证。例如，通过传递context.Context对象来显式退出\ngo func(ctx context.","title":"Golang内存优化实践指南"},{"content":"问题的发现 周五，本是一个风清气爽，令人愉悦的日子。我本还在美滋滋地等待着下班，然而天有不测，有用户反应容器日志看不到了，根据经验我知道，日志采集\u0026amp;收集链路上很可能又发生了阻塞。\n登录目标容器所在机器找到日志采集容器，并娴熟地敲下docker logs --tail 200 -f \u0026lt;container-id\u0026gt;命令，发现确实阻塞了，阻塞原因是上报日志的请求500了，从而不断重试导致日志采集阻塞。\n随后，我找到收集端的容器，查看日志，发现确实有请求报500了，并且抛出了Unknown value type的错误，查看相关代码。\n业务代码：\nif _, err := jsonparser.ArrayEach(body, func(value []byte, dataType jsonparser.ValueType, offset int, err error) {  ... }); err != nil {  return err // 错误抛出点 } jsonparser包中代码：\n显然问题出在了对body的解析上，究竟是什么样的body导致了解析错误呢？接下来，就是tcpdump和wireshark上场的时候了。\n使用Tcpdump抓包 首先，我们通过tcpdump抓到相关的请求。由于日志采集端会不断重试，因此最简单的方法便是登录采集端所在机器，并敲下如下命令tcpdump -i tunl0 dst port 7777 -w td.out ，并等待10-20秒。\n熟悉tcpdump的小伙伴，对这条命令显然已经心领神会了。尽管如此，这里我还是稍微解释下。\n -i tunl0：-i 参数用来指定网卡，由于采集器并没有通过eth0。因此，实战中，有时发现命令正确缺抓不到包的情况时，不妨指定下别的网卡。网络错综复杂，不一定都会通过eth0网卡。 dst port 777： 指定了目标端口作为过滤参数，收集端程序的端口号是7777 -w td.out: 表明将抓包记录保存在td.out文件中，这是因为json body是用base64编码并使用gzip加密后传输的，因此我得使用wireshark来抽离出来。（主要还是wireshark太香了:)，界面友好，操作简单，功能强大）  接着，我用scp命令将td.out文件拷到本地。并使用wireshar打开它\n使用Wireshark分析 打开后，首先映入眼帘的则是上图内容，看起来很多？不要慌，由于我们排查的是http请求，在过滤栏里输入HTTP，过滤掉非HTTP协议的记录。\n我们可以很清楚地发现，所有的HTTP都是发往一个IP的，且长度都是59，显然这些请求都是日志采集端程序不断重试的请求。接下来，我们只需要将某个请求里的body提取出来查看即可。\n很幸运，wireshark提供了这种功能，如上图所示，我们成功提取出来body内容。为bnVsbA==，使用base64解码后为null。\n解决问题 既然body的内容为null，那么调用jsonparser.ArrayEach报错也是意料之中的了，body内容必须得是一个JsonArray。\n然而，采集端为何会发送body为null的请求呢，深入源码，发现了如下一段逻辑。\nfunc (e *jsonEncoder) encode(obj []publisher.Event) (*bytes.Buffer, error) { \tvar events []map[string]interface{} \tfor _, o := range obj { \tm, err := transformMap(o) \tif err != nil { \tlogp.Err(\u0026#34;Fail to transform map with err: %s\u0026#34;, err) \tcontinue \t} \tevents = append(events, m) \t} \tdata, err := json.Marshal(events) \tif err != nil { \treturn nil, errors.Wrap(err, \u0026#34;fail to json marshal events\u0026#34;) \t}  ... } 由于transforMap函数回对obj中的元素进行转换，成功后添加到events中。\n但是，由于使用的是var events []map[string]interface{}这种声明方式，在Golang中，slice的零值为nil，因此events此时的值为nil。而当obj中所有的对象，被transforMap失败时，events使用json序列化后则为null了。\n这里我们需改变evetns的声明方式，使用events := make([]map[string]interface{}, 0)或者events := []map[string]interface{}{}的方式替代，此时events被初始化了，并指向的是一个cap为0的slice对象，其序列化后为[]。\n这样即使没有对象添加到events中，上报的也是一个空数组。\n参考  https://www.cnblogs.com/ggjucheng/archive/2012/01/14/2322659.html https://www.k0rz3n.com/2017/04/17/wireshark/  ","permalink":"https://erenming.github.io/posts/tcpdump-pricate-record/","summary":"问题的发现 周五，本是一个风清气爽，令人愉悦的日子。我本还在美滋滋地等待着下班，然而天有不测，有用户反应容器日志看不到了，根据经验我知道，日志采集\u0026amp;收集链路上很可能又发生了阻塞。\n登录目标容器所在机器找到日志采集容器，并娴熟地敲下docker logs --tail 200 -f \u0026lt;container-id\u0026gt;命令，发现确实阻塞了，阻塞原因是上报日志的请求500了，从而不断重试导致日志采集阻塞。\n随后，我找到收集端的容器，查看日志，发现确实有请求报500了，并且抛出了Unknown value type的错误，查看相关代码。\n业务代码：\nif _, err := jsonparser.ArrayEach(body, func(value []byte, dataType jsonparser.ValueType, offset int, err error) {  ... }); err != nil {  return err // 错误抛出点 } jsonparser包中代码：\n显然问题出在了对body的解析上，究竟是什么样的body导致了解析错误呢？接下来，就是tcpdump和wireshark上场的时候了。\n使用Tcpdump抓包 首先，我们通过tcpdump抓到相关的请求。由于日志采集端会不断重试，因此最简单的方法便是登录采集端所在机器，并敲下如下命令tcpdump -i tunl0 dst port 7777 -w td.out ，并等待10-20秒。\n熟悉tcpdump的小伙伴，对这条命令显然已经心领神会了。尽管如此，这里我还是稍微解释下。\n -i tunl0：-i 参数用来指定网卡，由于采集器并没有通过eth0。因此，实战中，有时发现命令正确缺抓不到包的情况时，不妨指定下别的网卡。网络错综复杂，不一定都会通过eth0网卡。 dst port 777： 指定了目标端口作为过滤参数，收集端程序的端口号是7777 -w td.out: 表明将抓包记录保存在td.out文件中，这是因为json body是用base64编码并使用gzip加密后传输的，因此我得使用wireshark来抽离出来。（主要还是wireshark太香了:)，界面友好，操作简单，功能强大）  接着，我用scp命令将td.out文件拷到本地。并使用wireshar打开它\n使用Wireshark分析 打开后，首先映入眼帘的则是上图内容，看起来很多？不要慌，由于我们排查的是http请求，在过滤栏里输入HTTP，过滤掉非HTTP协议的记录。\n我们可以很清楚地发现，所有的HTTP都是发往一个IP的，且长度都是59，显然这些请求都是日志采集端程序不断重试的请求。接下来，我们只需要将某个请求里的body提取出来查看即可。\n很幸运，wireshark提供了这种功能，如上图所示，我们成功提取出来body内容。为bnVsbA==，使用base64解码后为null。","title":"一次抓包排查实战记录"},{"content":"总所周知，大多数语言中，字典的底层是哈希表，而且其算法也是十分清晰的。无论采用链表法还是开放寻址法，我们都能实现一个简单的哈希表结构。对于Go来说，它是具体如何实现哈希表的呢？以及，采取了哪些优化策略呢？\n内存模型 map在内存的总体结构如下图所示。\n头部结构体hmap type hmap struct { \tcount int // 键值对个数 \tflags uint8 \tB uint8 // 2^B = 桶数量 \tnoverflow uint16 // 溢出桶的个数 \thash0 uint32 // hash seed  \tbuckets unsafe.Pointer // 哈希桶 \toldbuckets unsafe.Pointer // 原哈希桶，扩容时为非空 \tnevacuate uintptr // 扩容进度，地址小于它的桶已被迁移了  \textra *mapextra // optional fields } hmap即为map编译后的内存表示，这里需要注意的有两点。\n B的值是根据负载因子(LoadFactor)以及存储的键值对数量，在创建或扩容时动态改变 buckets是一个指针，它指向一个bmap结构  桶结构体bmap type bmap struct { \t// tophash数组可以看做键值对的索引 \ttophash [bucketCnt]uint8 \t// 实际上编译器会动态添加下述属性  // keys [8]keytype  // values [8]valuetype  // padding uinptr  // overflow uinptr } 虽然bmap结构体中只有一个tophash数组，但实际上，其后跟着8个key的槽位、8个value的槽位、padding以及一个overflow指针。如下图所示\n这里，Go做了优化。\n 这里并没有把key/value作为一个entry，而是分开存储。主要是为了节省内存，有时可以避免使用padding(额外的内存)来对齐，比如map[int64]int8就完全不需要padding。  查找操作 查找操作总体和链表法的哈希表查找类似，即key \u0026mdash;\u0026gt; hashFunc(key) \u0026mdash;\u0026gt; mask(hash) \u0026mdash;\u0026gt; 桶的位置 \u0026mdash;\u0026gt; 遍历链表。其主要代码如下所示\nfunc mapaccess1(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer { \t... \talg := t.key.alg \thash := alg.hash(key, uintptr(h.hash0)) \tm := bucketMask(h.B) \t// 计算得到桶的位置bucket-k \tb := (*bmap)(add(h.buckets, (hash\u0026amp;m)*uintptr(t.bucketsize)))  // 若正在扩容，老buckets则为非空 \t// 若bucket-k在老的buckets数组中，未被迁移，则使用老的 \tif c := h.oldbuckets; c != nil { \tif !h.sameSizeGrow() { \t// There used to be half as many buckets; mask down one more power of two. \tm \u0026gt;\u0026gt;= 1 \t} \toldb := (*bmap)(add(c, (hash\u0026amp;m)*uintptr(t.bucketsize))) \tif !evacuated(oldb) { \tb = oldb \t} \t} \t// 根据tophash(hash), 在bucket-k中的tophash中查找key \ttop := tophash(hash)  // 找到对应的bucket后，遍历查找对应的key/value bucketloop: \tfor ; b != nil; b = b.overflow(t) { \tfor i := uintptr(0); i \u0026lt; bucketCnt; i++ { \t... \t// 计算第i个位置的key的地址 \tk := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) \tif t.indirectkey() { \tk = *((*unsafe.Pointer)(k)) \t} \t// 比较tophash[i]上的k是否与目标key相等 \tif alg.equal(key, k) {  // 计算value的地址 \tv := add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.valuesize)) \tif t.indirectvalue() { \tv = *((*unsafe.Pointer)(v)) \t} \treturn v \t} \t} \t} \t// 若最终还是没找到，则返回nil \treturn unsafe.Pointer(\u0026amp;zeroVal[0]) } 首先，Go通过对应类型的alg.hash计算得到hash值（各种类型的hash\u0026amp;equal函数定义），取后B位作为buckets数组的下标(实际上为取余)，取高8位作为tophash的下标。\n然后，通过一个嵌套循环查找目标key：外层循环是遍历一个bmap单链表，它们通过overflow指针相连；内层循环则遍历tophash数组，逐个比较，当匹配成功时，则计算得到实际key的地址，比较两者，成功则返回。如下图所示\n这里，Go做了如下优化。\n 使用tophash数组，作为索引，用以判断key是否存在该bmap中，若确实存在，再使用较为耗时的比较算法判断key是否相等。  除了查找操作，map的插入、删除以及扩容操作也十分值得学习，大家可以去查阅相关源码\n本人才疏学浅，文章难免有些不足之处，非常欢迎大大们评论指出。\n参考  https://dave.cheney.net/2018/05/29/how-the-go-runtime-implements-maps-efficiently-without-generics#easy-footnote-1-3224 https://github.com/golang/go/blob/master/src/runtime/map.go https://studygolang.com/articles/25134 https://www.linkinstar.wiki/2019/06/03/golang/source-code/graphic-golang-map/ [https://github.com/qcrao/Go-Questions/blob/master/map/map%20%E7%9A%84%E5%BA%95%E5%B1%82%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%E6%98%AF%E4%BB%80%E4%B9%88.md](https://github.com/qcrao/Go-Questions/blob/master/map/map 的底层实现原理是什么.md)  ","permalink":"https://erenming.github.io/posts/map-in-go/","summary":"总所周知，大多数语言中，字典的底层是哈希表，而且其算法也是十分清晰的。无论采用链表法还是开放寻址法，我们都能实现一个简单的哈希表结构。对于Go来说，它是具体如何实现哈希表的呢？以及，采取了哪些优化策略呢？\n内存模型 map在内存的总体结构如下图所示。\n头部结构体hmap type hmap struct { \tcount int // 键值对个数 \tflags uint8 \tB uint8 // 2^B = 桶数量 \tnoverflow uint16 // 溢出桶的个数 \thash0 uint32 // hash seed  \tbuckets unsafe.Pointer // 哈希桶 \toldbuckets unsafe.Pointer // 原哈希桶，扩容时为非空 \tnevacuate uintptr // 扩容进度，地址小于它的桶已被迁移了  \textra *mapextra // optional fields } hmap即为map编译后的内存表示，这里需要注意的有两点。\n B的值是根据负载因子(LoadFactor)以及存储的键值对数量，在创建或扩容时动态改变 buckets是一个指针，它指向一个bmap结构  桶结构体bmap type bmap struct { \t// tophash数组可以看做键值对的索引 \ttophash [bucketCnt]uint8 \t// 实际上编译器会动态添加下述属性  // keys [8]keytype  // values [8]valuetype  // padding uinptr  // overflow uinptr } 虽然bmap结构体中只有一个tophash数组，但实际上，其后跟着8个key的槽位、8个value的槽位、padding以及一个overflow指针。如下图所示","title":"Golang中的map实现"},{"content":"在编写web应用中，我们常常会遇到这样的需求，比如，我们需要上报每个API的运行时间到运维监控系统。这时候你可以像下述代码一样将统计的逻辑写到每个路由函数中。\nfunc someApi(w http.ResponseWriter, r *http.Request) { \tstart := time.Now() \t// your logic \tmetrics.Upload(time.Since(start)) } 然而，这显然有悖DRY原则，我们需要将这些非业务逻辑剥离出来以实现解耦。这时候，中间件就能派上用场了，为了简单起见，我们这里将采用标准库net/http来实现。\n准备工作 func hello(w http.ResponseWriter, r *http.Request) { \tlog.Println(\u0026#34;execute hello func\u0026#34;) \tw.Write([]byte(\u0026#34;hello, world\u0026#34;)) }  func main() { \thttp.Handle(\u0026#34;/\u0026#34;, http.HandlerFunc(hello)) \thttp.ListenAndServe(\u0026#34;:3000\u0026#34;, nil) } 这里，我们创建了一个hello函数并将其转换成一个Handler用以处理HTTP请求。\n中间件的实现 func middlewareOne(next http.Handler) http.Handler { \treturn http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { \tlog.Println(\u0026#34;middleware one start\u0026#34;) // before logic \tnext.ServeHTTP(w, r) \tlog.Println(\u0026#34;middleware one end\u0026#34;) // after logic \t}) } 这里，我们实现了一个中间件函数middlewareOne，它接收一个next的参数其类型为http.Handler并返回一个新的http.Handler。而next.ServeHTTP(w, r)会回调next函数自身，即next(w, r)。看到这里，你可能会有点懵，我们需要先复习一下Handler，HandlerFunc，ServeHTTP三者的关系。\n下面是三者的定义：\n// A Handler responds to an HTTP request. type Handler interface { \tServeHTTP(ResponseWriter, *Request) }  type HandlerFunc func(ResponseWriter, *Request)  // ServeHTTP calls f(w, r). func (f HandlerFunc) ServeHTTP(w ResponseWriter, r *Request) { \tf(w, r) } Handler是一个接口，用以处理HTTP请求，换句话说，一个函数若想成为一个路由函数必须得是一个Handler接口。\nHandlerFunc是一个签名为func(ResponseWriter, *Request)的函数类型，其实现了ServerHTTP方法且签名相同，故HandleFunc是一个Handler，其ServerHTTP方法调用HandlerFunc自身。\n三者关系如下图所示：\n好了，接下来我们将中间件函数应用到我们的Handler上。\nfunc main() { \thttp.Handle(\u0026#34;/\u0026#34;, middlewareOne(http.HandlerFunc(hello))) \thttp.ListenAndServe(\u0026#34;:3000\u0026#34;, nil) } 运行程序，然后访问http://127.0.0.1:3000/，控制台将输出如下结果。\n2019/12/middleware one start 2019/12/execute hello func 2019/12/middleware one end 当然，如果你想应用多个中间件，你只需要再套上一层，例如下述代码：\nfunc middlewareTwo(next http.Handler) http.Handler { \treturn http.HandlerFunc(func (w http.ResponseWriter, r *http.Request) { \tlog.Println(\u0026#34;middleware two start\u0026#34;) \tnext.ServeHTTP(w, r) \tlog.Println(\u0026#34;middleware two end\u0026#34;) \t}) }  func main() { \thttp.Handle(\u0026#34;/\u0026#34;, middlewareTwo(middlewareOne(http.HandlerFunc(hello)))) \thttp.ListenAndServe(\u0026#34;:3000\u0026#34;, nil) } 我画出了该函数链的执行流程，如下图所示：\n可以看到，如果我们把路由函数hello看做为汉堡里的肉饼，中间件函数看做成面包。那么，middlewareOne包住了肉饼hello，而middlewareTwo又包住了middlewareTwo。\n总结  中间件函数与Python中的装饰器函数十分类似，都是在原函数逻辑不变的条件下进行扩展。 对于Web框架而言，类似于Flask里面的before_request和after_request钩子函数，next.ServeHTTP之前逻辑的等同于before_request里的逻辑、之后的等同于于after_request里的逻辑。 在业务开发中，我们应尽量将非业务逻辑抽象到中间件中，实现代码的松耦合与易扩展。  本人才疏学浅，文章难免有些不足之处，非常欢迎大家评论指出。\n参考  https://github.com/chai2010/advanced-go-programming-book/blob/master/ch5-web/ch5-03-middleware.md https://www.alexedwards.net/blog/making-and-using-middleware https://golang.org/doc/effective_go.html#interface_methods ","permalink":"https://erenming.github.io/posts/understand-middleware/","summary":"\u003cp\u003e在编写web应用中，我们常常会遇到这样的需求，比如，我们需要上报每个API的运行时间到运维监控系统。这时候你可以像下述代码一样将统计的逻辑写到每个路由函数中。\u003c/p\u003e","title":"用Golang实现并理解Web中间件"},{"content":"说到string类型，我们往往都能很熟练地对它进行各种处理，包括迭代、随机访问和匹配等等操作。然而在工作中，我发现迭代一个字符串产生的字符的类型与随机访问一个字符的类型却并不相同，为什么会这么奇怪呢？于是我决定一探究竟\nstring 简析 在Golang中，字符串本质上看一看做一个只读的字节切片(仅比切片少了一个Cap属性)。它的底层结构我们可以查看reflect.StringHeader得到:\ntype StringHeader struct {  Data uintptr  Len int } 例如针对字符串\u0026quot;你好\u0026quot;，其在内存中的表示如下图所示： Go的源文件默认使用UTF-8编码，所有的字符串字面量一般也是UTF-8编码的，故这里的你编码为\\xe4\\xbd\\xa0，好编码为\\xe5\\xa5\\xbd。UTF-8编码不是我们讨论的重点，具体可参考这篇博客。\n这里我们运行下述代码\n\ts := []byte{0xe4, 0xbd, 0xa0} \tfmt.Printf(\u0026#34;char is %s\u0026#34;, string(s)) 得到运行结果char is 你。\n虽然字符串并非切片，但是支持切片操作。对于同一字面量，不同的字符串变量指向相同的底层数组，这是因为字符串是只读的，为了节省内存，相同字面量的字符串通常对应于同一字符串常量。例如：\n\ts := \u0026#34;hello, world\u0026#34; \ts1 := \u0026#34;hello, world\u0026#34; \ts2 := \u0026#34;hello, world\u0026#34;[7:] \tfmt.Printf(\u0026#34;%d \\n\u0026#34;, (*reflect.StringHeader)(unsafe.Pointer(\u0026amp;s)).Data) // 17598361 \tfmt.Printf(\u0026#34;%d \\n\u0026#34;, (*reflect.StringHeader)(unsafe.Pointer(\u0026amp;s1)).Data) // 17598361 \tfmt.Printf(\u0026#34;%d \\n\u0026#34;, (*reflect.StringHeader)(unsafe.Pointer(\u0026amp;s2)).Data) // 17598368 可以看到，三者均指向同一个底层数组。对于s1, s2由于是同一字符串常量hello, world，故指向一个底层数组，以h为起始位置；而s2是字面量hello, world的切片操作后生成的字符串，也指向同一字节底层数组，不过是以w为起始位置。\n迭代字符串 当我们使用for range迭代字符串时，每次迭代Go都会用UTF-8解码出一个rune类型的字符，且索引为当前rune的起始位置(以字节为最下单位)。\n\tfor index, char := range \u0026#34;你好\u0026#34; { \tfmt.Printf(\u0026#34;start at %d, Unicode = %U, char = %c\\n\u0026#34;, index, char, char) \t} 得到运行结果\nstart at 0, Unicode = U+4F60, char = 你 start at 3, Unicode = U+597D, char = 好 随机访问字符串 当我们用下标访问字符串时，返回的值为单个字节，而我们直觉中，应该返回一个字符才合理。这还是因为string的后端数组是一个字节切片而非一个字符切片\n\ts := \u0026#34;你好\u0026#34; \tfmt.Printf(\u0026#34;s[%d] = %q, hex = %x, Unicode = %U\u0026#34;, 1, s[1], s[1], s[1]) 得到运行结果\ns[1] = \u0026#39;½\u0026#39;, hex = bd, Unicode = U+00BD 这里我们打印出来索引位置为1的字节，为0xbd，其Unicode为U+00BD, 代表的字符为½。(你可以通过这里查询)\n到底什么是rune、字符和字节 字节：即byte，它由8个位组成，即1byte = 8bit，是计算机中的基本计量单位，在Go中为byte类型，其实际上为uint8的别名\n字符：字符的概念比较模糊，在Unicode中通常用code point来表示。在我的理解里，是一种信息单元(例如一个符号、字母等)\nrune：其实际上是int32的别名，但是为了方便将字符与整数值区分开，从而新增了rune类型代表一个字符。\n总结  通过下标访问字符串时，返回的是一个字节，这往往与我们的直觉相背。所以，如果你一定要通过下标访问字符串，可以先将其转换为[]rune类型 字符串可以看做是一个只读字节切片, 支持切片操作。  本人才疏学浅，文章难免有些不足之处，非常欢迎大家评论指出。\n参考  https://chorer.github.io/2019/09/16/CB-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9Fcp1/ https://draveness.me/golang/datastructure/golang-string.html https://berryjam.github.io/2018/03/%E4%BB%8Egolang%E5%AD%97%E7%AC%A6%E4%B8%B2string%E9%81%8D%E5%8E%86%E8%AF%B4%E8%B5%B7/ https://en.wikipedia.org/wiki/Code_point https://github.com/chai2010/advanced-go-programming-book/blob/master/ch1-basic/ch1-03-array-string-and-slice.md ","permalink":"https://erenming.github.io/posts/string-in-golang/","summary":"\u003cp\u003e说到\u003ccode\u003estring\u003c/code\u003e类型，我们往往都能很熟练地对它进行各种处理，包括迭代、随机访问和匹配等等操作。然而在工作中，我发现迭代一个字符串产生的字符的类型与随机访问一个字符的类型却并不相同，为什么会这么奇怪呢？于是我决定一探究竟\u003c/p\u003e","title":"Golang中的string实现"},{"content":"仅以记录我在读、已读、预读之书。\n在读 22.06-22.12  《知识变现》：技术人还是看看，学习下如何提升影响力 《Observability Engineering》：关于可观测领域比较全面的书了，从业者可重点关注第1、2、4章节 《stop reading news》：确实很有道理，这年头新闻量与日俱增，浪费时间且易过度焦虑 《深入理解Kafka：核心设计与实践原理 第二版》：工作中常用Kafka，讲得还不错，有理论有实操（60%） 《ClickHouse原理解析与应用实践》：主要看了实现原理部分，讲的还是比较通俗易懂的，值得一读（50%） 《Systems Performance: Enterprise and the Cloud》：性能之巅第二版，[真]大头书，重要读了概念部分，实操部分待日后实践的时候再细读(60%) 《Streaming Processing with Apache Flink》： 工作中用到了，深入学习下 《如何阅读一本》：被推荐读一读，正好审视下目前的阅读方法 《重构：第二版》：重构方面的经典书  预读  《Programming Rust》：学习下最先进的Rust吧 《Queueing theory in Action》：排队理论，工作中队列的使用无处不在，需要学习下  已读  《Real-Time Analytics Techniques to Analyze》：70%   介绍了流式系统的常见设计和考虑，还是不错的，对我设计监控数据链路有帮助\n  《Kubernetes In Action》：100%   挺不错的一本书，Kubernetes的东西基本都涵盖了，其中的各种流程图有助于理解各组件的工作机制\n  《Designing Data Intensive Applications》：200%   读了两遍，强烈推荐，尤其是第一部分和第二部分。读完之后, 能对现如今的各种分布式数据组件的有更深的理解，配合Mit-6.824食用更佳\n  《Operating Systems: Three Easy Pieces》: 45%   主要看了CPU部分。以更为通俗易懂、诙谐的方式讲解操作系统，尤其是每章之后的教授问答环节。不过感觉还是作为教科书的目的编写的\n  《GO专家编程》：30%   华为大佬编写，主要对Go语言各种实现原理进行讲解。对实现能有大概的理解，不过也没有太深入细节，可以面试前看看。\n  《垃圾回收的算法与实现》：60%   介绍了各种GC算法和具体语言的实现，可以重点看看算法篇，对了解GC很有帮助，不过我当时还是看不太懂，可能需要再修炼修炼才能更好理解\n ","permalink":"https://erenming.github.io/readings/","summary":"仅以记录我在读、已读、预读之书。\n在读 22.06-22.12  《知识变现》：技术人还是看看，学习下如何提升影响力 《Observability Engineering》：关于可观测领域比较全面的书了，从业者可重点关注第1、2、4章节 《stop reading news》：确实很有道理，这年头新闻量与日俱增，浪费时间且易过度焦虑 《深入理解Kafka：核心设计与实践原理 第二版》：工作中常用Kafka，讲得还不错，有理论有实操（60%） 《ClickHouse原理解析与应用实践》：主要看了实现原理部分，讲的还是比较通俗易懂的，值得一读（50%） 《Systems Performance: Enterprise and the Cloud》：性能之巅第二版，[真]大头书，重要读了概念部分，实操部分待日后实践的时候再细读(60%) 《Streaming Processing with Apache Flink》： 工作中用到了，深入学习下 《如何阅读一本》：被推荐读一读，正好审视下目前的阅读方法 《重构：第二版》：重构方面的经典书  预读  《Programming Rust》：学习下最先进的Rust吧 《Queueing theory in Action》：排队理论，工作中队列的使用无处不在，需要学习下  已读  《Real-Time Analytics Techniques to Analyze》：70%   介绍了流式系统的常见设计和考虑，还是不错的，对我设计监控数据链路有帮助\n  《Kubernetes In Action》：100%   挺不错的一本书，Kubernetes的东西基本都涵盖了，其中的各种流程图有助于理解各组件的工作机制\n  《Designing Data Intensive Applications》：200%   读了两遍，强烈推荐，尤其是第一部分和第二部分。读完之后, 能对现如今的各种分布式数据组件的有更深的理解，配合Mit-6.824食用更佳\n  《Operating Systems: Three Easy Pieces》: 45%   主要看了CPU部分。以更为通俗易懂、诙谐的方式讲解操作系统，尤其是每章之后的教授问答环节。不过感觉还是作为教科书的目的编写的","title":""},{"content":"嗨，我的朋友！\n欢迎来到我的技术博客小屋，我是姜小明（erenming)。\n我的座右铭是Keep It Simple, Stupid，没错，就是大名鼎鼎的KISS原则啦！\n我是一名软件工程师，Go是我最喜欢的开发语言，专注于Cloud Native, Observability, Backend\n最后，欢迎与我联系，聊啥都行 :P\n","permalink":"https://erenming.github.io/about/","summary":"嗨，我的朋友！\n欢迎来到我的技术博客小屋，我是姜小明（erenming)。\n我的座右铭是Keep It Simple, Stupid，没错，就是大名鼎鼎的KISS原则啦！\n我是一名软件工程师，Go是我最喜欢的开发语言，专注于Cloud Native, Observability, Backend\n最后，欢迎与我联系，聊啥都行 :P","title":"About"}]